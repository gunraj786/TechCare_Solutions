{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet google-cloud-aiplatform google-cloud-storage vertexai pandas numpy langchain tiktoken python-dateutil sentence-transformers scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x11UzmPtsjJy",
        "outputId": "346fea54-7874-4294-83a7-c5f45333a731"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: google-cloud-aiplatform 1.71.1 does not provide the extra 'all'\u001b[0m\u001b[33m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-adk 1.13.0 requires google-cloud-aiplatform[agent-engines]<2.0.0,>=1.95.1, but you have google-cloud-aiplatform 1.71.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gcloud auth login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C2088pZ7uIhQ",
        "outputId": "f2d2f20b-b705-4dff-89a2-d3c094d4b919"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Go to the following link in your browser, and complete the sign-in prompts:\n",
            "\n",
            "    https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=32555940559.apps.googleusercontent.com&redirect_uri=https%3A%2F%2Fsdk.cloud.google.com%2Fauthcode.html&scope=openid+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fuserinfo.email+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcloud-platform+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fappengine.admin+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fsqlservice.login+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcompute+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Faccounts.reauth&state=1wqJ1m9qr6bhzKMGFyvGRmqmDJ4YBp&prompt=consent&token_usage=remote&access_type=offline&code_challenge=5cZGdOWUEjYeb2Zi6j9ADV5--Z_LLCmJDh3O4jRSeYU&code_challenge_method=S256\n",
            "\n",
            "Once finished, enter the verification code provided in your browser: 4/0AVMBsJgJKMSW5cAhnEAfmTcSKz0PCmFTgIMnxpq2xDQLUAl3evO3oQABc7QLcCzlPuKlVw\n",
            "\n",
            "You are now logged in as [jonahprashanth@gmail.com].\n",
            "Your current project is [i-monolith-468706-i9].  You can change this setting by running:\n",
            "  $ gcloud config set project PROJECT_ID\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gcloud config set project i-monolith-468706-i9"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6I1iiPphuvDz",
        "outputId": "20089147-68a8-475a-f663-2b0db0f84c37"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated property [core/project].\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gcloud auth list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AiwYrUXJu7f9",
        "outputId": "01e64a79-a3d8-4669-bd3e-f8fca128d570"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     Credentialed Accounts\n",
            "ACTIVE  ACCOUNT\n",
            "*       jonahprashanth@gmail.com\n",
            "\n",
            "To set the active account, run:\n",
            "    $ gcloud config set account `ACCOUNT`\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gcloud config list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nuooWAavu9Cn",
        "outputId": "de8282b0-4628-46cf-a9c8-6cb7718ee7f9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[component_manager]\n",
            "disable_update_check = True\n",
            "[compute]\n",
            "gce_metadata_read_timeout_sec = 0\n",
            "[core]\n",
            "account = jonahprashanth@gmail.com\n",
            "project = i-monolith-468706-i9\n",
            "\n",
            "Your active configuration is: [default]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gcloud iam service-accounts create vertex-sa \\\n",
        "  --display-name \"Vertex AI Service Account\" \\\n",
        "  --project i-monolith-468706-i9"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aHgCaae5vBao",
        "outputId": "6b98ce89-f337-4ba0-ab09-8f80049a4fc5"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created service account [vertex-sa].\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gcloud projects add-iam-policy-binding i-monolith-468706-i9 \\\n",
        "  --member=\"serviceAccount:vertex-sa@i-monolith-468706-i9.iam.gserviceaccount.com\" \\\n",
        "  --role=\"roles/aiplatform.user\"\n",
        "\n",
        "!gcloud projects add-iam-policy-binding i-monolith-468706-i9 \\\n",
        "  --member=\"serviceAccount:vertex-sa@i-monolith-468706-i9.iam.gserviceaccount.com\" \\\n",
        "  --role=\"roles/storage.objectAdmin\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SIfAIEuBvZm9",
        "outputId": "20605115-28f7-4288-c03d-f0834f0f4575"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated IAM policy for project [i-monolith-468706-i9].\n",
            "bindings:\n",
            "- members:\n",
            "  - serviceAccount:sample-test@i-monolith-468706-i9.iam.gserviceaccount.com\n",
            "  role: roles/aiplatform.admin\n",
            "- members:\n",
            "  - serviceAccount:service-393707886745@gcp-sa-aiplatform.iam.gserviceaccount.com\n",
            "  role: roles/aiplatform.serviceAgent\n",
            "- members:\n",
            "  - serviceAccount:sample-test@i-monolith-468706-i9.iam.gserviceaccount.com\n",
            "  - serviceAccount:vertex-sa@i-monolith-468706-i9.iam.gserviceaccount.com\n",
            "  - user:jonahprashanth@gmail.com\n",
            "  role: roles/aiplatform.user\n",
            "- members:\n",
            "  - serviceAccount:service-393707886745@gcp-sa-artifactregistry.iam.gserviceaccount.com\n",
            "  role: roles/artifactregistry.serviceAgent\n",
            "- members:\n",
            "  - serviceAccount:393707886745@cloudbuild.gserviceaccount.com\n",
            "  role: roles/cloudbuild.builds.builder\n",
            "- members:\n",
            "  - serviceAccount:service-393707886745@gcp-sa-cloudbuild.iam.gserviceaccount.com\n",
            "  role: roles/cloudbuild.serviceAgent\n",
            "- members:\n",
            "  - serviceAccount:service-393707886745@compute-system.iam.gserviceaccount.com\n",
            "  role: roles/compute.serviceAgent\n",
            "- members:\n",
            "  - serviceAccount:service-393707886745@containerregistry.iam.gserviceaccount.com\n",
            "  role: roles/containerregistry.ServiceAgent\n",
            "- members:\n",
            "  - serviceAccount:service-393707886745@dataproc-accounts.iam.gserviceaccount.com\n",
            "  role: roles/dataproc.serviceAgent\n",
            "- members:\n",
            "  - serviceAccount:393707886745-compute@developer.gserviceaccount.com\n",
            "  - serviceAccount:393707886745@cloudservices.gserviceaccount.com\n",
            "  role: roles/editor\n",
            "- members:\n",
            "  - serviceAccount:service-393707886745@gcp-sa-notebooks.iam.gserviceaccount.com\n",
            "  role: roles/notebooks.serviceAgent\n",
            "- members:\n",
            "  - user:jonahprashanth@gmail.com\n",
            "  role: roles/owner\n",
            "- members:\n",
            "  - serviceAccount:service-393707886745@gcp-sa-pubsub.iam.gserviceaccount.com\n",
            "  role: roles/pubsub.serviceAgent\n",
            "- members:\n",
            "  - serviceAccount:service-393707886745@serverless-robot-prod.iam.gserviceaccount.com\n",
            "  role: roles/run.serviceAgent\n",
            "- members:\n",
            "  - user:jonahprashanth@gmail.com\n",
            "  role: roles/storage.objectAdmin\n",
            "etag: BwY-gb2CruE=\n",
            "version: 1\n",
            "Updated IAM policy for project [i-monolith-468706-i9].\n",
            "bindings:\n",
            "- members:\n",
            "  - serviceAccount:sample-test@i-monolith-468706-i9.iam.gserviceaccount.com\n",
            "  role: roles/aiplatform.admin\n",
            "- members:\n",
            "  - serviceAccount:service-393707886745@gcp-sa-aiplatform.iam.gserviceaccount.com\n",
            "  role: roles/aiplatform.serviceAgent\n",
            "- members:\n",
            "  - serviceAccount:sample-test@i-monolith-468706-i9.iam.gserviceaccount.com\n",
            "  - serviceAccount:vertex-sa@i-monolith-468706-i9.iam.gserviceaccount.com\n",
            "  - user:jonahprashanth@gmail.com\n",
            "  role: roles/aiplatform.user\n",
            "- members:\n",
            "  - serviceAccount:service-393707886745@gcp-sa-artifactregistry.iam.gserviceaccount.com\n",
            "  role: roles/artifactregistry.serviceAgent\n",
            "- members:\n",
            "  - serviceAccount:393707886745@cloudbuild.gserviceaccount.com\n",
            "  role: roles/cloudbuild.builds.builder\n",
            "- members:\n",
            "  - serviceAccount:service-393707886745@gcp-sa-cloudbuild.iam.gserviceaccount.com\n",
            "  role: roles/cloudbuild.serviceAgent\n",
            "- members:\n",
            "  - serviceAccount:service-393707886745@compute-system.iam.gserviceaccount.com\n",
            "  role: roles/compute.serviceAgent\n",
            "- members:\n",
            "  - serviceAccount:service-393707886745@containerregistry.iam.gserviceaccount.com\n",
            "  role: roles/containerregistry.ServiceAgent\n",
            "- members:\n",
            "  - serviceAccount:service-393707886745@dataproc-accounts.iam.gserviceaccount.com\n",
            "  role: roles/dataproc.serviceAgent\n",
            "- members:\n",
            "  - serviceAccount:393707886745-compute@developer.gserviceaccount.com\n",
            "  - serviceAccount:393707886745@cloudservices.gserviceaccount.com\n",
            "  role: roles/editor\n",
            "- members:\n",
            "  - serviceAccount:service-393707886745@gcp-sa-notebooks.iam.gserviceaccount.com\n",
            "  role: roles/notebooks.serviceAgent\n",
            "- members:\n",
            "  - user:jonahprashanth@gmail.com\n",
            "  role: roles/owner\n",
            "- members:\n",
            "  - serviceAccount:service-393707886745@gcp-sa-pubsub.iam.gserviceaccount.com\n",
            "  role: roles/pubsub.serviceAgent\n",
            "- members:\n",
            "  - serviceAccount:service-393707886745@serverless-robot-prod.iam.gserviceaccount.com\n",
            "  role: roles/run.serviceAgent\n",
            "- members:\n",
            "  - serviceAccount:vertex-sa@i-monolith-468706-i9.iam.gserviceaccount.com\n",
            "  - user:jonahprashanth@gmail.com\n",
            "  role: roles/storage.objectAdmin\n",
            "etag: BwY-gb2i3OM=\n",
            "version: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gcloud services enable aiplatform.googleapis.com\n",
        "!gcloud services enable storage.googleapis.com"
      ],
      "metadata": {
        "id": "JE93SylctB8P"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gcloud iam service-accounts keys create key.json \\\n",
        "  --iam-account=vertex-sa@i-monolith-468706-i9.iam.gserviceaccount.com\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TWHJtVpGve2V",
        "outputId": "e7e2f2c6-5386-4162-f585-7c72a63cd0eb"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "created key [8be270412f1ca8482bb84050b786bfa5ab63f7b5] of type [json] as [key.json] for [vertex-sa@i-monolith-468706-i9.iam.gserviceaccount.com]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"key.json\"\n"
      ],
      "metadata": {
        "id": "P9d12KFEvs8_"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from datetime import datetime\n",
        "from typing import List, Dict, Any, Optional\n",
        "import ast\n",
        "import warnings\n",
        "import time\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from google.cloud import storage\n",
        "import vertexai\n",
        "from vertexai.preview.language_models import TextEmbeddingModel\n",
        "from vertexai.generative_models import GenerativeModel\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configuration\n",
        "PROJECT_ID = \"i-monolith-468706-i9\"\n",
        "LOCATION = \"us-central1\"\n",
        "BUCKET_NAME = \"bucket_jonah\"  # Update with your bucket name\n",
        "\n",
        "# Initialize Vertex AI\n",
        "vertexai.init(project=PROJECT_ID, location=LOCATION)\n",
        "\n",
        "class MedicalDataProcessor:\n",
        "    \"\"\"Handles CSV data cleaning and preprocessing\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=1500,  # 1.5KB chunks\n",
        "            chunk_overlap=200,\n",
        "            separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
        "        )\n",
        "\n",
        "    def clean_csv_data(self, csv_path: str) -> pd.DataFrame:\n",
        "        \"\"\"Clean and preprocess the EHR CSV data\"\"\"\n",
        "        print(\"Loading CSV data...\")\n",
        "        df = pd.read_csv(csv_path)\n",
        "\n",
        "        print(f\"Original data shape: {df.shape}\")\n",
        "\n",
        "        # Basic cleaning\n",
        "        df = df.dropna(subset=['TEXT'])  # Remove rows without medical text\n",
        "        df = df.drop_duplicates(subset=['TEXT'])  # Remove duplicate notes\n",
        "\n",
        "        # Clean text content\n",
        "        df['TEXT'] = df['TEXT'].apply(self._clean_medical_text)\n",
        "\n",
        "        # Parse JSON fields safely\n",
        "        df['diagnoses_parsed'] = df['diagnoses'].apply(self._safe_parse_json)\n",
        "        df['procedures_parsed'] = df['procedures'].apply(self._safe_parse_json)\n",
        "        df['cpt_codes_parsed'] = df['cpt_codes'].apply(self._safe_parse_json)\n",
        "\n",
        "        # Standardize dates\n",
        "        df['CHARTDATE'] = pd.to_datetime(df['CHARTDATE'], errors='coerce')\n",
        "\n",
        "        # Remove rows with no medical codes\n",
        "        df = df[\n",
        "            (df['diagnoses_parsed'].notna()) |\n",
        "            (df['procedures_parsed'].notna()) |\n",
        "            (df['cpt_codes_parsed'].notna())\n",
        "        ]\n",
        "\n",
        "        print(f\"Cleaned data shape: {df.shape}\")\n",
        "        return df\n",
        "\n",
        "    def _clean_medical_text(self, text: str) -> str:\n",
        "        \"\"\"Clean medical text content\"\"\"\n",
        "        if pd.isna(text) or not isinstance(text, str):\n",
        "            return \"\"\n",
        "\n",
        "        # Remove common PHI patterns (basic anonymization)\n",
        "        text = re.sub(r'\\[\\*\\*.*?\\*\\*\\]', '[REDACTED]', text)  # Remove bracketed PHI\n",
        "\n",
        "        # Normalize whitespace\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "        # Remove control characters\n",
        "        text = ''.join(char for char in text if ord(char) >= 32 or char == '\\n')\n",
        "\n",
        "        return text\n",
        "\n",
        "    def _safe_parse_json(self, json_str: str) -> Optional[List[Dict]]:\n",
        "        \"\"\"Safely parse JSON strings from CSV\"\"\"\n",
        "        if pd.isna(json_str) or json_str == '':\n",
        "            return None\n",
        "\n",
        "        try:\n",
        "            # Handle string representation of list\n",
        "            if isinstance(json_str, str):\n",
        "                # Try ast.literal_eval first\n",
        "                parsed = ast.literal_eval(json_str)\n",
        "                return parsed if isinstance(parsed, list) else None\n",
        "        except (ValueError, SyntaxError):\n",
        "            try:\n",
        "                # Try json.loads as backup\n",
        "                return json.loads(json_str)\n",
        "            except json.JSONDecodeError:\n",
        "                return None\n",
        "        return None\n",
        "\n",
        "    def _extract_codes_as_strings(self, codes_list: Optional[List[Dict]], code_key: str) -> List[str]:\n",
        "        \"\"\"Extract codes from parsed JSON and convert to strings\"\"\"\n",
        "        if not codes_list:\n",
        "            return []\n",
        "\n",
        "        result = []\n",
        "        for item in codes_list:\n",
        "            if isinstance(item, dict) and code_key in item:\n",
        "                code = item[code_key]\n",
        "                if code is not None and str(code).strip():\n",
        "                    result.append(str(code).strip())\n",
        "        return result\n",
        "\n",
        "    def _extract_titles_as_strings(self, codes_list: Optional[List[Dict]], title_key: str) -> List[str]:\n",
        "        \"\"\"Extract titles from parsed JSON and convert to strings\"\"\"\n",
        "        if not codes_list:\n",
        "            return []\n",
        "\n",
        "        result = []\n",
        "        for item in codes_list:\n",
        "            if isinstance(item, dict) and title_key in item:\n",
        "                title = item[title_key]\n",
        "                if title is not None and str(title).strip():\n",
        "                    result.append(str(title).strip())\n",
        "        return result\n",
        "\n",
        "    def chunk_documents(self, df: pd.DataFrame) -> List[Dict[str, Any]]:\n",
        "        \"\"\"Split long medical texts into chunks\"\"\"\n",
        "        chunks = []\n",
        "\n",
        "        for idx, row in df.iterrows():\n",
        "            text = row['TEXT']\n",
        "            if not text or len(text.strip()) < 50:\n",
        "                continue\n",
        "\n",
        "            # Split text into chunks\n",
        "            text_chunks = self.text_splitter.split_text(text)\n",
        "\n",
        "            for chunk_idx, chunk in enumerate(text_chunks):\n",
        "                chunk_data = {\n",
        "                    'content': chunk,\n",
        "                    'original_row_id': int(row['row_id']),\n",
        "                    'subject_id': int(row['SUBJECT_ID']),\n",
        "                    'admission_id': int(row['HADM_ID']),\n",
        "                    'chart_date': row['CHARTDATE'].isoformat() if pd.notna(row['CHARTDATE']) else None,\n",
        "                    'category': str(row['CATEGORY']),\n",
        "                    'description': str(row['DESCRIPTION']),\n",
        "                    'chunk_id': chunk_idx,\n",
        "                    'total_chunks': len(text_chunks),\n",
        "\n",
        "                    # Medical codes\n",
        "                    'diagnoses': row['diagnoses_parsed'] or [],\n",
        "                    'procedures': row['procedures_parsed'] or [],\n",
        "                    'cpt_codes': row['cpt_codes_parsed'] or [],\n",
        "\n",
        "                    # Extract code strings properly\n",
        "                    'diagnosis_codes': self._extract_codes_as_strings(row['diagnoses_parsed'], 'ICD9_CODE'),\n",
        "                    'diagnosis_titles': self._extract_titles_as_strings(row['diagnoses_parsed'], 'LONG_TITLE'),\n",
        "                    'procedure_codes': self._extract_codes_as_strings(row['procedures_parsed'], 'ICD9_CODE'),\n",
        "                    'procedure_titles': self._extract_titles_as_strings(row['procedures_parsed'], 'LONG_TITLE'),\n",
        "                    'cpt_code_values': self._extract_codes_as_strings(row['cpt_codes_parsed'], 'CPT_CD'),\n",
        "                    'cpt_descriptions': self._extract_titles_as_strings(row['cpt_codes_parsed'], 'DESCRIPTION')\n",
        "                }\n",
        "                chunks.append(chunk_data)\n",
        "\n",
        "        print(f\"Created {len(chunks)} text chunks\")\n",
        "        return chunks\n",
        "\n",
        "class VertexAIMedicalRAG:\n",
        "    \"\"\"Medical RAG system using Google Cloud Storage and Vertex AI\"\"\"\n",
        "\n",
        "    def __init__(self, project_id: str, location: str, bucket_name: str):\n",
        "        self.project_id = project_id\n",
        "        self.location = location\n",
        "        self.bucket_name = bucket_name\n",
        "\n",
        "        # Initialize Vertex AI\n",
        "        vertexai.init(project=project_id, location=location)\n",
        "\n",
        "        # Initialize models\n",
        "        self.embedding_model = TextEmbeddingModel.from_pretrained(\"text-embedding-004\")\n",
        "        self.generative_model = GenerativeModel(\"gemini-1.5-flash-002\")\n",
        "\n",
        "        # Initialize GCS client\n",
        "        self.storage_client = storage.Client(project=project_id)\n",
        "        self.bucket = self.storage_client.bucket(bucket_name)\n",
        "\n",
        "        # In-memory storage for embeddings (loaded from GCS)\n",
        "        self.medical_records = []\n",
        "\n",
        "        print(f\"✅ Initialized Vertex AI Medical RAG\")\n",
        "        print(f\"   Project: {project_id}\")\n",
        "        print(f\"   Location: {location}\")\n",
        "        print(f\"   Bucket: {bucket_name}\")\n",
        "\n",
        "    def generate_embeddings(self, texts: List[str], batch_size: int = 32) -> List[List[float]]:\n",
        "        \"\"\"Generate embeddings using Vertex AI in batches\"\"\"\n",
        "        all_embeddings = []\n",
        "\n",
        "        print(f\"Generating embeddings for {len(texts)} texts in batches of {batch_size}...\")\n",
        "\n",
        "        for i in range(0, len(texts), batch_size):\n",
        "            batch = texts[i:i+batch_size]\n",
        "            try:\n",
        "                embeddings = self.embedding_model.get_embeddings(batch)\n",
        "                batch_embeddings = [e.values for e in embeddings]\n",
        "                all_embeddings.extend(batch_embeddings)\n",
        "\n",
        "                print(f\"  Processed batch {i//batch_size + 1}/{(len(texts)-1)//batch_size + 1}\")\n",
        "                time.sleep(0.5)  # Rate limiting\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"❌ Error in batch {i//batch_size + 1}: {e}\")\n",
        "                # Add empty embeddings for failed batch\n",
        "                empty_embeddings = [[0.0] * 768 for _ in batch]  # text-embedding-004 has 768 dimensions\n",
        "                all_embeddings.extend(empty_embeddings)\n",
        "\n",
        "        return all_embeddings\n",
        "\n",
        "    def create_embeddings_and_store(self, chunks: List[Dict[str, Any]]):\n",
        "        \"\"\"Create embeddings for chunks and store in GCS\"\"\"\n",
        "        print(\"=== EMBEDDING CREATION PHASE ===\")\n",
        "\n",
        "        # Extract texts for embedding\n",
        "        texts = [chunk['content'] for chunk in chunks]\n",
        "\n",
        "        # Generate embeddings\n",
        "        embeddings = self.generate_embeddings(texts)\n",
        "\n",
        "        # Combine chunks with embeddings\n",
        "        medical_records = []\n",
        "        for chunk, embedding in zip(chunks, embeddings):\n",
        "            record = {\n",
        "                **chunk,  # Include all chunk data\n",
        "                'embedding': embedding,\n",
        "                'embedding_id': f\"{chunk['original_row_id']}_{chunk['chunk_id']}\"\n",
        "            }\n",
        "            medical_records.append(record)\n",
        "\n",
        "        # Save to local JSONL first\n",
        "        os.makedirs(\"embeddings\", exist_ok=True)\n",
        "        local_path = \"embeddings/medical_embeddings.jsonl\"\n",
        "\n",
        "        with open(local_path, \"w\") as f:\n",
        "            for record in medical_records:\n",
        "                f.write(json.dumps(record, default=str) + \"\\n\")\n",
        "\n",
        "        print(f\"✅ Saved {len(medical_records)} records to {local_path}\")\n",
        "\n",
        "        # Upload to GCS\n",
        "        gcs_path = \"embeddings/medical_embeddings.jsonl\"\n",
        "        blob = self.bucket.blob(gcs_path)\n",
        "        blob.upload_from_filename(local_path)\n",
        "\n",
        "        print(f\"⬆️ Uploaded to gs://{self.bucket_name}/{gcs_path}\")\n",
        "\n",
        "        # Store in memory for immediate use\n",
        "        self.medical_records = medical_records\n",
        "        print(f\"✅ Loaded {len(self.medical_records)} records into memory\")\n",
        "\n",
        "    def load_embeddings_from_gcs(self):\n",
        "        \"\"\"Load embeddings from GCS into memory\"\"\"\n",
        "        try:\n",
        "            gcs_path = \"embeddings/medical_embeddings.jsonl\"\n",
        "            blob = self.bucket.blob(gcs_path)\n",
        "\n",
        "            # Download to local file\n",
        "            local_path = \"/tmp/medical_embeddings.jsonl\"\n",
        "            blob.download_to_filename(local_path)\n",
        "\n",
        "            # Load into memory\n",
        "            self.medical_records = []\n",
        "            with open(local_path, \"r\") as f:\n",
        "                for line in f:\n",
        "                    record = json.loads(line.strip())\n",
        "                    self.medical_records.append(record)\n",
        "\n",
        "            print(f\"✅ Loaded {len(self.medical_records)} records from GCS\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error loading from GCS: {e}\")\n",
        "            print(\"Please run embedding creation first.\")\n",
        "\n",
        "    def search_similar_records(self, query: str, top_k: int = 10) -> List[Dict[str, Any]]:\n",
        "        \"\"\"Search for similar medical records using cosine similarity\"\"\"\n",
        "        if not self.medical_records:\n",
        "            print(\"❌ No medical records loaded. Please load embeddings first.\")\n",
        "            return []\n",
        "\n",
        "        print(f\"🔍 Searching for: '{query}'\")\n",
        "\n",
        "        # Generate query embedding\n",
        "        query_embeddings = self.embedding_model.get_embeddings([query])\n",
        "        query_vector = np.array([query_embeddings[0].values])\n",
        "\n",
        "        # Get all record embeddings\n",
        "        record_embeddings = np.array([record['embedding'] for record in self.medical_records])\n",
        "\n",
        "        # Calculate similarities\n",
        "        similarities = cosine_similarity(query_vector, record_embeddings)[0]\n",
        "\n",
        "        # Get top-k similar records\n",
        "        top_indices = np.argsort(similarities)[::-1][:top_k]\n",
        "\n",
        "        similar_records = []\n",
        "        for idx in top_indices:\n",
        "            record = self.medical_records[idx].copy()\n",
        "            record['similarity_score'] = float(similarities[idx])\n",
        "            similar_records.append(record)\n",
        "\n",
        "        return similar_records\n",
        "\n",
        "    def extract_medical_codes_from_results(self, similar_records: List[Dict[str, Any]]) -> Dict[str, Dict]:\n",
        "        \"\"\"Extract and aggregate medical codes from search results\"\"\"\n",
        "        medical_codes = {\n",
        "            'diagnoses': {},\n",
        "            'procedures': {},\n",
        "            'cpt_codes': {}\n",
        "        }\n",
        "\n",
        "        for record in similar_records:\n",
        "            # Process diagnoses\n",
        "            for diag in record.get('diagnoses', []):\n",
        "                code = diag.get('ICD9_CODE')\n",
        "                if code:\n",
        "                    if code not in medical_codes['diagnoses']:\n",
        "                        medical_codes['diagnoses'][code] = {\n",
        "                            'short_title': diag.get('SHORT_TITLE', ''),\n",
        "                            'long_title': diag.get('LONG_TITLE', ''),\n",
        "                            'frequency': 0,\n",
        "                            'max_similarity': 0\n",
        "                        }\n",
        "                    medical_codes['diagnoses'][code]['frequency'] += 1\n",
        "                    medical_codes['diagnoses'][code]['max_similarity'] = max(\n",
        "                        medical_codes['diagnoses'][code]['max_similarity'],\n",
        "                        record['similarity_score']\n",
        "                    )\n",
        "\n",
        "            # Process procedures\n",
        "            for proc in record.get('procedures', []):\n",
        "                code = str(proc.get('ICD9_CODE'))\n",
        "                if code and code != 'None':\n",
        "                    if code not in medical_codes['procedures']:\n",
        "                        medical_codes['procedures'][code] = {\n",
        "                            'short_title': proc.get('SHORT_TITLE', ''),\n",
        "                            'long_title': proc.get('LONG_TITLE', ''),\n",
        "                            'frequency': 0,\n",
        "                            'max_similarity': 0\n",
        "                        }\n",
        "                    medical_codes['procedures'][code]['frequency'] += 1\n",
        "                    medical_codes['procedures'][code]['max_similarity'] = max(\n",
        "                        medical_codes['procedures'][code]['max_similarity'],\n",
        "                        record['similarity_score']\n",
        "                    )\n",
        "\n",
        "            # Process CPT codes\n",
        "            for cpt in record.get('cpt_codes', []):\n",
        "                code = cpt.get('CPT_CD')\n",
        "                if code:\n",
        "                    if code not in medical_codes['cpt_codes']:\n",
        "                        medical_codes['cpt_codes'][code] = {\n",
        "                            'description': cpt.get('DESCRIPTION', ''),\n",
        "                            'section': cpt.get('SECTIONHEADER', ''),\n",
        "                            'subsection': cpt.get('SUBSECTIONHEADER', ''),\n",
        "                            'frequency': 0,\n",
        "                            'max_similarity': 0\n",
        "                        }\n",
        "                    medical_codes['cpt_codes'][code]['frequency'] += 1\n",
        "                    medical_codes['cpt_codes'][code]['max_similarity'] = max(\n",
        "                        medical_codes['cpt_codes'][code]['max_similarity'],\n",
        "                        record['similarity_score']\n",
        "                    )\n",
        "\n",
        "        return medical_codes\n",
        "\n",
        "    def generate_medical_response(self, query: str, similar_records: List[Dict[str, Any]], medical_codes: Dict[str, Dict]) -> str:\n",
        "        \"\"\"Generate explanation using Vertex AI Gemini\"\"\"\n",
        "\n",
        "        # Prepare context from top records\n",
        "        context = \"\\n\".join([\n",
        "            f\"Record {i+1}: {record['content'][:300]}...\"\n",
        "            for i, record in enumerate(similar_records[:3])\n",
        "        ])\n",
        "\n",
        "        # Format medical codes\n",
        "        diagnoses_text = \"\"\n",
        "        if medical_codes['diagnoses']:\n",
        "            diagnoses_text = \"DIAGNOSIS CODES FOUND:\\n\"\n",
        "            for code, info in sorted(medical_codes['diagnoses'].items(),\n",
        "                                   key=lambda x: x[1]['max_similarity'], reverse=True)[:5]:\n",
        "                diagnoses_text += f\"- {code}: {info['long_title']} (frequency: {info['frequency']}, similarity: {info['max_similarity']:.3f})\\n\"\n",
        "\n",
        "        procedures_text = \"\"\n",
        "        if medical_codes['procedures']:\n",
        "            procedures_text = \"PROCEDURE CODES FOUND:\\n\"\n",
        "            for code, info in sorted(medical_codes['procedures'].items(),\n",
        "                                   key=lambda x: x[1]['max_similarity'], reverse=True)[:5]:\n",
        "                procedures_text += f\"- {code}: {info['long_title']} (frequency: {info['frequency']}, similarity: {info['max_similarity']:.3f})\\n\"\n",
        "\n",
        "        cpt_text = \"\"\n",
        "        if medical_codes['cpt_codes']:\n",
        "            cpt_text = \"CPT CODES FOUND:\\n\"\n",
        "            for code, info in sorted(medical_codes['cpt_codes'].items(),\n",
        "                                   key=lambda x: x[1]['max_similarity'], reverse=True)[:5]:\n",
        "                cpt_text += f\"- {code}: {info['description']} (frequency: {info['frequency']}, similarity: {info['max_similarity']:.3f})\\n\"\n",
        "\n",
        "        prompt = f\"\"\"\n",
        "You are a medical coding assistant. Based on the following medical records context and the user's query about diseases/symptoms, provide relevant medical codes and explanations.\n",
        "\n",
        "CONTEXT FROM MEDICAL RECORDS:\n",
        "{context}\n",
        "\n",
        "{diagnoses_text}\n",
        "\n",
        "{procedures_text}\n",
        "\n",
        "{cpt_text}\n",
        "\n",
        "USER QUERY: {query}\n",
        "\n",
        "Please provide a comprehensive response that:\n",
        "1. Explains what medical codes are most relevant to the query\n",
        "2. Provides context on why these codes match the query\n",
        "3. Lists the specific ICD9 diagnosis codes, procedure codes, and CPT codes\n",
        "4. Explains what each code represents in simple terms\n",
        "5. Notes the similarity scores to show relevance\n",
        "\n",
        "Keep your response concise and well-formatted.\n",
        "\"\"\"\n",
        "\n",
        "        try:\n",
        "            response = self.generative_model.generate_content(prompt)\n",
        "            return response.text\n",
        "        except Exception as e:\n",
        "            return f\"Error generating explanation: {e}. However, medical codes were found successfully.\"\n",
        "\n",
        "    def query(self, user_query: str, top_k: int = 10) -> Dict[str, Any]:\n",
        "        \"\"\"Main query interface\"\"\"\n",
        "        print(f\"Processing query: {user_query}\")\n",
        "\n",
        "        # Search for similar records\n",
        "        similar_records = self.search_similar_records(user_query, top_k)\n",
        "\n",
        "        if not similar_records:\n",
        "            return {\n",
        "                'query': user_query,\n",
        "                'explanation': \"No similar medical records found.\",\n",
        "                'medical_codes': {'diagnoses': {}, 'procedures': {}, 'cpt_codes': {}},\n",
        "                'retrieved_records_count': 0,\n",
        "                'sample_records': []\n",
        "            }\n",
        "\n",
        "        # Extract medical codes\n",
        "        medical_codes = self.extract_medical_codes_from_results(similar_records)\n",
        "\n",
        "        # Generate explanation\n",
        "        explanation = self.generate_medical_response(user_query, similar_records, medical_codes)\n",
        "\n",
        "        # Prepare sample records\n",
        "        sample_records = []\n",
        "        for record in similar_records[:3]:\n",
        "            sample_records.append({\n",
        "                'content': record['content'][:200] + \"...\" if len(record['content']) > 200 else record['content'],\n",
        "                'category': record['category'],\n",
        "                'description': record['description'],\n",
        "                'similarity_score': record['similarity_score'],\n",
        "                'row_id': record['original_row_id']\n",
        "            })\n",
        "\n",
        "        return {\n",
        "            'query': user_query,\n",
        "            'explanation': explanation,\n",
        "            'medical_codes': medical_codes,\n",
        "            'retrieved_records_count': len(similar_records),\n",
        "            'sample_records': sample_records\n",
        "        }"
      ],
      "metadata": {
        "id": "hWePOQSXwTVa"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def main():\n",
        "    \"\"\"Main execution function\"\"\"\n",
        "\n",
        "    # Configuration\n",
        "    CSV_PATH = \"/content/ehr_records.csv\"  # Update with your CSV path\n",
        "\n",
        "    # Initialize components\n",
        "    processor = MedicalDataProcessor()\n",
        "    rag_system = VertexAIMedicalRAG(PROJECT_ID, LOCATION, BUCKET_NAME)\n",
        "\n",
        "    # Setup mode\n",
        "    setup_mode = input(\"Do you want to setup the database? (y/n): \").lower() == 'y'\n",
        "\n",
        "    if setup_mode:\n",
        "        print(\"=== DATA PROCESSING PHASE ===\")\n",
        "        df = processor.clean_csv_data(CSV_PATH)\n",
        "        chunks = processor.chunk_documents(df)\n",
        "\n",
        "        print(\"=== EMBEDDING & STORAGE PHASE ===\")\n",
        "        rag_system.create_embeddings_and_store(chunks)\n",
        "        print(\"Setup complete!\")\n",
        "    else:\n",
        "        # Load existing embeddings\n",
        "        print(\"=== LOADING EMBEDDINGS FROM GCS ===\")\n",
        "        rag_system.load_embeddings_from_gcs()\n",
        "\n",
        "    # Interactive query mode\n",
        "    print(\"\\n=== MEDICAL RAG SYSTEM READY ===\")\n",
        "    print(\"Enter medical queries to find relevant codes. Type 'quit' to exit.\\n\")\n",
        "\n",
        "    # Sample queries\n",
        "    sample_queries = [\n",
        "        \"chest pain and shortness of breath\",\n",
        "        \"cardiac arrest and heart failure\",\n",
        "        \"difficulty breathing\",\n",
        "        \"hypotension and shock\",\n",
        "        \"mechanical ventilation\"\n",
        "    ]\n",
        "\n",
        "    print(\"Sample queries you can try:\")\n",
        "    for i, q in enumerate(sample_queries, 1):\n",
        "        print(f\"  {i}. {q}\")\n",
        "    print()\n",
        "\n",
        "    while True:\n",
        "        query = input(\"Enter your medical query (or 'quit' to exit): \").strip()\n",
        "        if query.lower() in ['quit', 'exit', 'q']:\n",
        "            break\n",
        "\n",
        "        if not query:\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            results = rag_system.query(query)\n",
        "\n",
        "            print(\"\\n\" + \"=\"*60)\n",
        "            print(f\"QUERY: {results['query']}\")\n",
        "            print(\"=\"*60)\n",
        "\n",
        "            print(\"🏥 MEDICAL CODES FOUND:\")\n",
        "\n",
        "            # Display diagnoses\n",
        "            if results['medical_codes']['diagnoses']:\n",
        "                print(\"\\n📋 DIAGNOSIS CODES (ICD9):\")\n",
        "                for code, info in results['medical_codes']['diagnoses'].items():\n",
        "                    print(f\"  • {code}: {info['long_title']} (freq: {info['frequency']}, sim: {info.get('max_similarity', 0):.3f})\")\n",
        "\n",
        "            # Display procedures\n",
        "            if results['medical_codes']['procedures']:\n",
        "                print(\"\\n🔬 PROCEDURE CODES (ICD9):\")\n",
        "                for code, info in results['medical_codes']['procedures'].items():\n",
        "                    print(f\"  • {code}: {info['long_title']} (freq: {info['frequency']}, sim: {info.get('max_similarity', 0):.3f})\")\n",
        "\n",
        "            # Display CPT codes\n",
        "            if results['medical_codes']['cpt_codes']:\n",
        "                print(\"\\n💊 CPT CODES:\")\n",
        "                for code, info in results['medical_codes']['cpt_codes'].items():\n",
        "                    print(f\"  • {code}: {info['description']} (freq: {info['frequency']}, sim: {info.get('max_similarity', 0):.3f})\")\n",
        "\n",
        "            if not any([\n",
        "                results['medical_codes']['diagnoses'],\n",
        "                results['medical_codes']['procedures'],\n",
        "                results['medical_codes']['cpt_codes']\n",
        "            ]):\n",
        "                print(\"  No specific medical codes found for this query.\")\n",
        "\n",
        "            print(f\"\\n📄 EXPLANATION:\")\n",
        "            print(results['explanation'])\n",
        "\n",
        "            print(f\"\\n📊 Retrieved {results['retrieved_records_count']} relevant medical records\")\n",
        "            print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error processing query: {e}\")\n",
        "            print(\"Please try again with a different query.\\n\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sa1QZSKXwVJ9",
        "outputId": "5e1d0878-7f73-48f9-911f-d33aa456476d"
      },
      "execution_count": 20,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Initialized Vertex AI Medical RAG\n",
            "   Project: i-monolith-468706-i9\n",
            "   Location: us-central1\n",
            "   Bucket: bucket_jonah\n",
            "Do you want to setup the database? (y/n): y\n",
            "=== DATA PROCESSING PHASE ===\n",
            "Loading CSV data...\n",
            "Original data shape: (1976, 14)\n",
            "Cleaned data shape: (1945, 17)\n",
            "Created 3710 text chunks\n",
            "=== EMBEDDING & STORAGE PHASE ===\n",
            "=== EMBEDDING CREATION PHASE ===\n",
            "Generating embeddings for 3710 texts in batches of 32...\n",
            "  Processed batch 1/116\n",
            "  Processed batch 2/116\n",
            "  Processed batch 3/116\n",
            "  Processed batch 4/116\n",
            "  Processed batch 5/116\n",
            "  Processed batch 6/116\n",
            "  Processed batch 7/116\n",
            "  Processed batch 8/116\n",
            "  Processed batch 9/116\n",
            "  Processed batch 10/116\n",
            "  Processed batch 11/116\n",
            "  Processed batch 12/116\n",
            "  Processed batch 13/116\n",
            "  Processed batch 14/116\n",
            "  Processed batch 15/116\n",
            "  Processed batch 16/116\n",
            "  Processed batch 17/116\n",
            "  Processed batch 18/116\n",
            "  Processed batch 19/116\n",
            "  Processed batch 20/116\n",
            "  Processed batch 21/116\n",
            "  Processed batch 22/116\n",
            "  Processed batch 23/116\n",
            "  Processed batch 24/116\n",
            "  Processed batch 25/116\n",
            "  Processed batch 26/116\n",
            "  Processed batch 27/116\n",
            "  Processed batch 28/116\n",
            "  Processed batch 29/116\n",
            "  Processed batch 30/116\n",
            "  Processed batch 31/116\n",
            "  Processed batch 32/116\n",
            "  Processed batch 33/116\n",
            "  Processed batch 34/116\n",
            "  Processed batch 35/116\n",
            "  Processed batch 36/116\n",
            "  Processed batch 37/116\n",
            "  Processed batch 38/116\n",
            "  Processed batch 39/116\n",
            "  Processed batch 40/116\n",
            "  Processed batch 41/116\n",
            "  Processed batch 42/116\n",
            "  Processed batch 43/116\n",
            "  Processed batch 44/116\n",
            "  Processed batch 45/116\n",
            "  Processed batch 46/116\n",
            "  Processed batch 47/116\n",
            "  Processed batch 48/116\n",
            "  Processed batch 49/116\n",
            "  Processed batch 50/116\n",
            "  Processed batch 51/116\n",
            "  Processed batch 52/116\n",
            "  Processed batch 53/116\n",
            "  Processed batch 54/116\n",
            "  Processed batch 55/116\n",
            "  Processed batch 56/116\n",
            "  Processed batch 57/116\n",
            "  Processed batch 58/116\n",
            "  Processed batch 59/116\n",
            "  Processed batch 60/116\n",
            "  Processed batch 61/116\n",
            "  Processed batch 62/116\n",
            "  Processed batch 63/116\n",
            "  Processed batch 64/116\n",
            "  Processed batch 65/116\n",
            "  Processed batch 66/116\n",
            "  Processed batch 67/116\n",
            "  Processed batch 68/116\n",
            "  Processed batch 69/116\n",
            "  Processed batch 70/116\n",
            "  Processed batch 71/116\n",
            "  Processed batch 72/116\n",
            "  Processed batch 73/116\n",
            "  Processed batch 74/116\n",
            "  Processed batch 75/116\n",
            "  Processed batch 76/116\n",
            "  Processed batch 77/116\n",
            "  Processed batch 78/116\n",
            "  Processed batch 79/116\n",
            "  Processed batch 80/116\n",
            "  Processed batch 81/116\n",
            "  Processed batch 82/116\n",
            "  Processed batch 83/116\n",
            "  Processed batch 84/116\n",
            "  Processed batch 85/116\n",
            "  Processed batch 86/116\n",
            "  Processed batch 87/116\n",
            "  Processed batch 88/116\n",
            "  Processed batch 89/116\n",
            "  Processed batch 90/116\n",
            "  Processed batch 91/116\n",
            "  Processed batch 92/116\n",
            "  Processed batch 93/116\n",
            "  Processed batch 94/116\n",
            "  Processed batch 95/116\n",
            "  Processed batch 96/116\n",
            "  Processed batch 97/116\n",
            "  Processed batch 98/116\n",
            "  Processed batch 99/116\n",
            "  Processed batch 100/116\n",
            "  Processed batch 101/116\n",
            "  Processed batch 102/116\n",
            "  Processed batch 103/116\n",
            "  Processed batch 104/116\n",
            "  Processed batch 105/116\n",
            "  Processed batch 106/116\n",
            "  Processed batch 107/116\n",
            "  Processed batch 108/116\n",
            "  Processed batch 109/116\n",
            "  Processed batch 110/116\n",
            "  Processed batch 111/116\n",
            "  Processed batch 112/116\n",
            "  Processed batch 113/116\n",
            "  Processed batch 114/116\n",
            "  Processed batch 115/116\n",
            "  Processed batch 116/116\n",
            "✅ Saved 3710 records to embeddings/medical_embeddings.jsonl\n",
            "⬆️ Uploaded to gs://bucket_jonah/embeddings/medical_embeddings.jsonl\n",
            "✅ Loaded 3710 records into memory\n",
            "Setup complete!\n",
            "\n",
            "=== MEDICAL RAG SYSTEM READY ===\n",
            "Enter medical queries to find relevant codes. Type 'quit' to exit.\n",
            "\n",
            "Sample queries you can try:\n",
            "  1. chest pain and shortness of breath\n",
            "  2. cardiac arrest and heart failure\n",
            "  3. difficulty breathing\n",
            "  4. hypotension and shock\n",
            "  5. mechanical ventilation\n",
            "\n",
            "Enter your medical query (or 'quit' to exit): chest pain and shortness of breath\n",
            "Processing query: chest pain and shortness of breath\n",
            "🔍 Searching for: 'chest pain and shortness of breath'\n",
            "\n",
            "============================================================\n",
            "QUERY: chest pain and shortness of breath\n",
            "============================================================\n",
            "🏥 MEDICAL CODES FOUND:\n",
            "\n",
            "📋 DIAGNOSIS CODES (ICD9):\n",
            "  • 99681: Complications of transplanted kidney (freq: 1, sim: 0.591)\n",
            "  • 42833: Acute on chronic diastolic heart failure (freq: 1, sim: 0.591)\n",
            "  • 41071: Subendocardial infarction, initial episode of care (freq: 1, sim: 0.591)\n",
            "  • 40311: Hypertensive chronic kidney disease, benign, with chronic kidney disease stage V or end stage renal disease (freq: 1, sim: 0.591)\n",
            "  • 25200: Hyperparathyroidism, unspecified (freq: 1, sim: 0.591)\n",
            "  • 5856: End stage renal disease (freq: 1, sim: 0.591)\n",
            "  • 2749: Gout, unspecified (freq: 1, sim: 0.591)\n",
            "  • 27800: Obesity, unspecified (freq: 1, sim: 0.591)\n",
            "  • 2811: Other vitamin B12 deficiency anemia (freq: 1, sim: 0.591)\n",
            "  • 4280: Congestive heart failure, unspecified (freq: 1, sim: 0.591)\n",
            "  • E8780: Surgical operation with transplant of whole organ causing abnormal patient reaction, or later complication, without mention of misadventure at time of operation (freq: 1, sim: 0.591)\n",
            "  • 41401: Coronary atherosclerosis of native coronary artery (freq: 1, sim: 0.591)\n",
            "  • 28521: Anemia in chronic kidney disease (freq: 1, sim: 0.591)\n",
            "  • 2767: Hyperpotassemia (freq: 1, sim: 0.591)\n",
            "  • V1005: Personal history of malignant neoplasm of large intestine (freq: 1, sim: 0.591)\n",
            "\n",
            "🔬 PROCEDURE CODES (ICD9):\n",
            "  • 9390: Non-invasive mechanical ventilation (freq: 9, sim: 0.653)\n",
            "  • 9904: Transfusion of packed cells (freq: 9, sim: 0.653)\n",
            "  • 3995: Hemodialysis (freq: 10, sim: 0.653)\n",
            "  • 8867: Phlebography of other specified sites using contrast material (freq: 2, sim: 0.608)\n",
            "  • 5491: Percutaneous abdominal drainage (freq: 2, sim: 0.608)\n",
            "\n",
            "📄 EXPLANATION:\n",
            "Error generating explanation: 404 Publisher Model `projects/i-monolith-468706-i9/locations/us-central1/publishers/google/models/gemini-1.5-flash-002` was not found or your project does not have access to it. Please ensure you are using a valid model version. For more information, see: https://cloud.google.com/vertex-ai/generative-ai/docs/learn/model-versions. However, medical codes were found successfully.\n",
            "\n",
            "📊 Retrieved 10 relevant medical records\n",
            "============================================================\n",
            "\n",
            "Enter your medical query (or 'quit' to exit): quit\n"
          ]
        }
      ]
    }
  ]
}