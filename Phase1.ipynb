{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_AS-_VLXOTmz",
        "outputId": "91eecacf-685d-439c-a087-2b87c3e84970"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Cleaned dataset saved: 4709 rows\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import hashlib\n",
        "from datetime import datetime\n",
        "\n",
        "# ========================\n",
        "# ðŸ”¹ Helper Functions\n",
        "# ========================\n",
        "\n",
        "def anonymize_id(val):\n",
        "    \"\"\"Hash identifiers to anonymize PHI\"\"\"\n",
        "    if pd.isna(val):\n",
        "        return None\n",
        "    return hashlib.sha256(str(val).encode()).hexdigest()[:12]  # short surrogate\n",
        "\n",
        "def standardize_date(val):\n",
        "    \"\"\"Convert date/time to ISO format\"\"\"\n",
        "    if pd.isna(val):\n",
        "        return None\n",
        "    try:\n",
        "        return pd.to_datetime(val, errors=\"coerce\").strftime(\"%Y-%m-%d\")\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "def remove_phi_from_text(text):\n",
        "    \"\"\"Remove PHI patterns from TEXT field\"\"\"\n",
        "    if pd.isna(text):\n",
        "        return \"\"\n",
        "\n",
        "    # Regex patterns for PHI-like data\n",
        "    text = re.sub(r\"\\b\\d{3}-\\d{2}-\\d{4}\\b\", \"[SSN]\", text)  # SSN\n",
        "    text = re.sub(r\"\\b\\d{10}\\b\", \"[PHONE]\", text)            # 10-digit phone\n",
        "    text = re.sub(r\"\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b\", \"[EMAIL]\", text)  # email\n",
        "    text = re.sub(r\"\\b\\d{2}/\\d{2}/\\d{4}\\b\", \"[DATE]\", text)  # dates\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()  # normalize whitespace\n",
        "    return text\n",
        "\n",
        "def chunk_text(text, chunk_size=1024, overlap=100):\n",
        "    \"\"\"Split long text into ~1KB chunks with slight overlap\"\"\"\n",
        "    if not text:\n",
        "        return []\n",
        "    tokens = text.split()\n",
        "    chunks, start = [], 0\n",
        "    while start < len(tokens):\n",
        "        end = start + chunk_size // 6  # approx 6 chars per token\n",
        "        chunk = \" \".join(tokens[start:end])\n",
        "        chunks.append(chunk)\n",
        "        start = end - (overlap // 6)\n",
        "    return chunks\n",
        "\n",
        "# ========================\n",
        "# ðŸ”¹ Load and Clean Data\n",
        "# ========================\n",
        "\n",
        "df = pd.read_csv(\"/content/ehr_records.csv\")\n",
        "\n",
        "# 1. Deduplication\n",
        "df.drop_duplicates(inplace=True)\n",
        "\n",
        "# 2. Anonymize identifiers\n",
        "df[\"SUBJECT_ID\"] = df[\"SUBJECT_ID\"].apply(anonymize_id)\n",
        "df[\"HADM_ID\"] = df[\"HADM_ID\"].apply(anonymize_id)\n",
        "df[\"CGID\"] = df[\"CGID\"].apply(anonymize_id)\n",
        "\n",
        "# 3. Standardize date fields\n",
        "for col in [\"CHARTDATE\", \"CHARTTIME\", \"STORETIME\"]:\n",
        "    if col in df.columns:\n",
        "        df[col] = df[col].apply(standardize_date)\n",
        "\n",
        "# 4. Normalize CATEGORY & DESCRIPTION\n",
        "df[\"CATEGORY\"] = df[\"CATEGORY\"].str.strip().str.title()\n",
        "df[\"DESCRIPTION\"] = df[\"DESCRIPTION\"].astype(str).str.strip()\n",
        "\n",
        "# 5. Clean TEXT (PHI removal + whitespace normalization)\n",
        "df[\"TEXT\"] = df[\"TEXT\"].apply(remove_phi_from_text)\n",
        "\n",
        "# 6. Validate JSON-like fields\n",
        "for col in [\"diagnoses\", \"procedures\", \"cpt_codes\"]:\n",
        "    if col in df.columns:\n",
        "        df[col] = df[col].fillna(\"\").astype(str).str.replace(\"'\", '\"')\n",
        "\n",
        "# 7. Chunk long notes\n",
        "expanded_rows = []\n",
        "for _, row in df.iterrows():\n",
        "    chunks = chunk_text(row[\"TEXT\"])\n",
        "    if not chunks:\n",
        "        expanded_rows.append(row.to_dict())\n",
        "    else:\n",
        "        for i, chunk in enumerate(chunks):\n",
        "            r = row.to_dict()\n",
        "            r[\"TEXT\"] = chunk\n",
        "            r[\"chunk_id\"] = i\n",
        "            expanded_rows.append(r)\n",
        "\n",
        "df_cleaned = pd.DataFrame(expanded_rows)\n",
        "\n",
        "# ========================\n",
        "# ðŸ”¹ Save Cleaned Data\n",
        "# ========================\n",
        "\n",
        "df_cleaned.to_csv(\"cleaned_records.csv\", index=False)\n",
        "print(f\"âœ… Cleaned dataset saved: {df_cleaned.shape[0]} rows\")\n"
      ]
    }
  ]
}