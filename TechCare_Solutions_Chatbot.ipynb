{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet google-cloud-aiplatform google-cloud-storage vertexai pandas numpy langchain tiktoken python-dateutil sentence-transformers scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x11UzmPtsjJy",
        "outputId": "6ec2da34-7f62-43cc-ed25-08d2e60796d4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: google-cloud-aiplatform 1.71.1 does not provide the extra 'all'\u001b[0m\u001b[33m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m45.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-adk 1.13.0 requires google-cloud-aiplatform[agent-engines]<2.0.0,>=1.95.1, but you have google-cloud-aiplatform 1.71.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gcloud auth login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C2088pZ7uIhQ",
        "outputId": "4b74dc83-a8da-43db-eb8e-996d68169aec"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Go to the following link in your browser, and complete the sign-in prompts:\n",
            "\n",
            "    https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=32555940559.apps.googleusercontent.com&redirect_uri=https%3A%2F%2Fsdk.cloud.google.com%2Fauthcode.html&scope=openid+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fuserinfo.email+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcloud-platform+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fappengine.admin+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fsqlservice.login+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcompute+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Faccounts.reauth&state=NzT1dvq5f9FJsZjpVmDNTilmYcnwoY&prompt=consent&token_usage=remote&access_type=offline&code_challenge=hvaY2GMdjxz8xwWbfSf7AIE4qV-nBeMSWMLRNby1AAQ&code_challenge_method=S256\n",
            "\n",
            "Once finished, enter the verification code provided in your browser: 4/0AVMBsJihXIDjvKzypPLtR2vB9vE1H6SWWxnt3n0EkAwxthsILaK3Tl35FCJcFtz7_03hLA\n",
            "\n",
            "You are now logged in as [jonahprashanth@gmail.com].\n",
            "Your current project is [None].  You can change this setting by running:\n",
            "  $ gcloud config set project PROJECT_ID\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gcloud config set project i-monolith-468706-i9"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6I1iiPphuvDz",
        "outputId": "d0d2792a-f206-46de-c272-d093881f0a86"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated property [core/project].\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gcloud auth list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AiwYrUXJu7f9",
        "outputId": "732553e9-7d17-4418-d6eb-d611549a943e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     Credentialed Accounts\n",
            "ACTIVE  ACCOUNT\n",
            "*       jonahprashanth@gmail.com\n",
            "\n",
            "To set the active account, run:\n",
            "    $ gcloud config set account `ACCOUNT`\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gcloud config list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nuooWAavu9Cn",
        "outputId": "d53002bc-050d-4ab1-8e5c-6dbd2c8e8e92"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[component_manager]\n",
            "disable_update_check = True\n",
            "[compute]\n",
            "gce_metadata_read_timeout_sec = 0\n",
            "[core]\n",
            "account = jonahprashanth@gmail.com\n",
            "project = i-monolith-468706-i9\n",
            "\n",
            "Your active configuration is: [default]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gcloud iam service-accounts create vertex-sa \\\n",
        "  --display-name \"Vertex AI Service Account\" \\\n",
        "  --project i-monolith-468706-i9"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aHgCaae5vBao",
        "outputId": "9a20584f-ec46-47c0-f634-58b5c01261f3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;31mERROR:\u001b[0m (gcloud.iam.service-accounts.create) Resource in projects [i-monolith-468706-i9] is the subject of a conflict: Service account vertex-sa already exists within project projects/i-monolith-468706-i9.\n",
            "- '@type': type.googleapis.com/google.rpc.ResourceInfo\n",
            "  resourceName: projects/i-monolith-468706-i9/serviceAccounts/vertex-sa@i-monolith-468706-i9.iam.gserviceaccount.com\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gcloud projects add-iam-policy-binding i-monolith-468706-i9 \\\n",
        "  --member=\"serviceAccount:vertex-sa@i-monolith-468706-i9.iam.gserviceaccount.com\" \\\n",
        "  --role=\"roles/aiplatform.user\"\n",
        "\n",
        "!gcloud projects add-iam-policy-binding i-monolith-468706-i9 \\\n",
        "  --member=\"serviceAccount:vertex-sa@i-monolith-468706-i9.iam.gserviceaccount.com\" \\\n",
        "  --role=\"roles/storage.objectAdmin\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SIfAIEuBvZm9",
        "outputId": "be57eef1-1a58-4ca7-f761-53463a036d84"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated IAM policy for project [i-monolith-468706-i9].\n",
            "bindings:\n",
            "- members:\n",
            "  - serviceAccount:sample-test@i-monolith-468706-i9.iam.gserviceaccount.com\n",
            "  role: roles/aiplatform.admin\n",
            "- members:\n",
            "  - serviceAccount:service-393707886745@gcp-sa-aiplatform.iam.gserviceaccount.com\n",
            "  role: roles/aiplatform.serviceAgent\n",
            "- members:\n",
            "  - serviceAccount:sample-test@i-monolith-468706-i9.iam.gserviceaccount.com\n",
            "  - serviceAccount:vertex-sa@i-monolith-468706-i9.iam.gserviceaccount.com\n",
            "  - user:jonahprashanth@gmail.com\n",
            "  role: roles/aiplatform.user\n",
            "- members:\n",
            "  - serviceAccount:service-393707886745@gcp-sa-artifactregistry.iam.gserviceaccount.com\n",
            "  role: roles/artifactregistry.serviceAgent\n",
            "- members:\n",
            "  - serviceAccount:393707886745@cloudbuild.gserviceaccount.com\n",
            "  role: roles/cloudbuild.builds.builder\n",
            "- members:\n",
            "  - serviceAccount:service-393707886745@gcp-sa-cloudbuild.iam.gserviceaccount.com\n",
            "  role: roles/cloudbuild.serviceAgent\n",
            "- members:\n",
            "  - serviceAccount:service-393707886745@compute-system.iam.gserviceaccount.com\n",
            "  role: roles/compute.serviceAgent\n",
            "- members:\n",
            "  - serviceAccount:service-393707886745@containerregistry.iam.gserviceaccount.com\n",
            "  role: roles/containerregistry.ServiceAgent\n",
            "- members:\n",
            "  - serviceAccount:service-393707886745@dataproc-accounts.iam.gserviceaccount.com\n",
            "  role: roles/dataproc.serviceAgent\n",
            "- members:\n",
            "  - serviceAccount:393707886745-compute@developer.gserviceaccount.com\n",
            "  - serviceAccount:393707886745@cloudservices.gserviceaccount.com\n",
            "  role: roles/editor\n",
            "- members:\n",
            "  - serviceAccount:service-393707886745@gcp-sa-notebooks.iam.gserviceaccount.com\n",
            "  role: roles/notebooks.serviceAgent\n",
            "- members:\n",
            "  - user:jonahprashanth@gmail.com\n",
            "  role: roles/owner\n",
            "- members:\n",
            "  - serviceAccount:service-393707886745@gcp-sa-pubsub.iam.gserviceaccount.com\n",
            "  role: roles/pubsub.serviceAgent\n",
            "- members:\n",
            "  - serviceAccount:service-393707886745@serverless-robot-prod.iam.gserviceaccount.com\n",
            "  role: roles/run.serviceAgent\n",
            "- members:\n",
            "  - serviceAccount:vertex-sa@i-monolith-468706-i9.iam.gserviceaccount.com\n",
            "  - user:jonahprashanth@gmail.com\n",
            "  role: roles/storage.objectAdmin\n",
            "etag: BwY-wJPaxHA=\n",
            "version: 1\n",
            "Updated IAM policy for project [i-monolith-468706-i9].\n",
            "bindings:\n",
            "- members:\n",
            "  - serviceAccount:sample-test@i-monolith-468706-i9.iam.gserviceaccount.com\n",
            "  role: roles/aiplatform.admin\n",
            "- members:\n",
            "  - serviceAccount:service-393707886745@gcp-sa-aiplatform.iam.gserviceaccount.com\n",
            "  role: roles/aiplatform.serviceAgent\n",
            "- members:\n",
            "  - serviceAccount:sample-test@i-monolith-468706-i9.iam.gserviceaccount.com\n",
            "  - serviceAccount:vertex-sa@i-monolith-468706-i9.iam.gserviceaccount.com\n",
            "  - user:jonahprashanth@gmail.com\n",
            "  role: roles/aiplatform.user\n",
            "- members:\n",
            "  - serviceAccount:service-393707886745@gcp-sa-artifactregistry.iam.gserviceaccount.com\n",
            "  role: roles/artifactregistry.serviceAgent\n",
            "- members:\n",
            "  - serviceAccount:393707886745@cloudbuild.gserviceaccount.com\n",
            "  role: roles/cloudbuild.builds.builder\n",
            "- members:\n",
            "  - serviceAccount:service-393707886745@gcp-sa-cloudbuild.iam.gserviceaccount.com\n",
            "  role: roles/cloudbuild.serviceAgent\n",
            "- members:\n",
            "  - serviceAccount:service-393707886745@compute-system.iam.gserviceaccount.com\n",
            "  role: roles/compute.serviceAgent\n",
            "- members:\n",
            "  - serviceAccount:service-393707886745@containerregistry.iam.gserviceaccount.com\n",
            "  role: roles/containerregistry.ServiceAgent\n",
            "- members:\n",
            "  - serviceAccount:service-393707886745@dataproc-accounts.iam.gserviceaccount.com\n",
            "  role: roles/dataproc.serviceAgent\n",
            "- members:\n",
            "  - serviceAccount:393707886745-compute@developer.gserviceaccount.com\n",
            "  - serviceAccount:393707886745@cloudservices.gserviceaccount.com\n",
            "  role: roles/editor\n",
            "- members:\n",
            "  - serviceAccount:service-393707886745@gcp-sa-notebooks.iam.gserviceaccount.com\n",
            "  role: roles/notebooks.serviceAgent\n",
            "- members:\n",
            "  - user:jonahprashanth@gmail.com\n",
            "  role: roles/owner\n",
            "- members:\n",
            "  - serviceAccount:service-393707886745@gcp-sa-pubsub.iam.gserviceaccount.com\n",
            "  role: roles/pubsub.serviceAgent\n",
            "- members:\n",
            "  - serviceAccount:service-393707886745@serverless-robot-prod.iam.gserviceaccount.com\n",
            "  role: roles/run.serviceAgent\n",
            "- members:\n",
            "  - serviceAccount:vertex-sa@i-monolith-468706-i9.iam.gserviceaccount.com\n",
            "  - user:jonahprashanth@gmail.com\n",
            "  role: roles/storage.objectAdmin\n",
            "etag: BwY-wJQAlXA=\n",
            "version: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gcloud services enable aiplatform.googleapis.com\n",
        "!gcloud services enable storage.googleapis.com"
      ],
      "metadata": {
        "id": "JE93SylctB8P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6648ed56-d5f1-4b07-a460-2446bdcfffdb"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Operation \"operations/acat.p2-393707886745-d5446926-2edb-4b49-8d00-25ced7a86204\" finished successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gcloud iam service-accounts keys create key.json \\\n",
        "  --iam-account=vertex-sa@i-monolith-468706-i9.iam.gserviceaccount.com\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TWHJtVpGve2V",
        "outputId": "e56aae03-b013-4b72-cfbc-ad50058118a1"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "created key [058d8da7223d9970a6d3743ed590e919d2185fba] of type [json] as [key.json] for [vertex-sa@i-monolith-468706-i9.iam.gserviceaccount.com]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"key.json\"\n"
      ],
      "metadata": {
        "id": "P9d12KFEvs8_"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PROJECT_ID = \"i-monolith-468706-i9\"   # your GCP project\n",
        "BUCKET_NAME = \"bucket_jonah\"          # change if you want a new bucket\n",
        "LOCATION = \"us-central1\"              # or us-east1, asia-south1, etc.\n",
        "\n",
        "# --- 2. Create bucket (only if not already exists) ---\n",
        "!gcloud storage buckets create gs://$BUCKET_NAME \\\n",
        "    --project=$PROJECT_ID \\\n",
        "    --location=$LOCATION \\\n",
        "    --default-storage-class=STANDARD \\\n",
        "    --uniform-bucket-level-access \\\n",
        "    --quiet || echo \"Bucket may already exist\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Gt4cnFdF8zY",
        "outputId": "9367f5a3-051b-4b1f-8b0d-22e2808539f5"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating gs://bucket_jonah/...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from datetime import datetime\n",
        "from typing import List, Dict, Any, Optional\n",
        "import ast\n",
        "import warnings\n",
        "import time\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from google.cloud import storage\n",
        "import vertexai\n",
        "from vertexai.preview.language_models import TextEmbeddingModel\n",
        "from vertexai.generative_models import GenerativeModel\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configuration\n",
        "PROJECT_ID = \"i-monolith-468706-i9\"\n",
        "LOCATION = \"us-central1\"\n",
        "BUCKET_NAME = \"bucket_jonah\"  # Update with your bucket name\n",
        "\n",
        "# Initialize Vertex AI\n",
        "vertexai.init(project=PROJECT_ID, location=LOCATION)\n",
        "\n",
        "class MedicalDataProcessor:\n",
        "    \"\"\"Handles CSV data cleaning and preprocessing\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=1500,  # 1.5KB chunks\n",
        "            chunk_overlap=200,\n",
        "            separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
        "        )\n",
        "\n",
        "    def clean_csv_data(self, csv_path: str) -> pd.DataFrame:\n",
        "        \"\"\"Clean and preprocess the EHR CSV data\"\"\"\n",
        "        print(\"Loading CSV data...\")\n",
        "        df = pd.read_csv(csv_path)\n",
        "\n",
        "        print(f\"Original data shape: {df.shape}\")\n",
        "\n",
        "        # Basic cleaning\n",
        "        df = df.dropna(subset=['TEXT'])  # Remove rows without medical text\n",
        "        df = df.drop_duplicates(subset=['TEXT'])  # Remove duplicate notes\n",
        "\n",
        "        # Clean text content\n",
        "        df['TEXT'] = df['TEXT'].apply(self._clean_medical_text)\n",
        "\n",
        "        # Parse JSON fields safely\n",
        "        df['diagnoses_parsed'] = df['diagnoses'].apply(self._safe_parse_json)\n",
        "        df['procedures_parsed'] = df['procedures'].apply(self._safe_parse_json)\n",
        "        df['cpt_codes_parsed'] = df['cpt_codes'].apply(self._safe_parse_json)\n",
        "\n",
        "        # Standardize dates\n",
        "        df['CHARTDATE'] = pd.to_datetime(df['CHARTDATE'], errors='coerce')\n",
        "\n",
        "        # Remove rows with no medical codes\n",
        "        df = df[\n",
        "            (df['diagnoses_parsed'].notna()) |\n",
        "            (df['procedures_parsed'].notna()) |\n",
        "            (df['cpt_codes_parsed'].notna())\n",
        "        ]\n",
        "\n",
        "        print(f\"Cleaned data shape: {df.shape}\")\n",
        "        return df\n",
        "\n",
        "    def _clean_medical_text(self, text: str) -> str:\n",
        "        \"\"\"Clean medical text content\"\"\"\n",
        "        if pd.isna(text) or not isinstance(text, str):\n",
        "            return \"\"\n",
        "\n",
        "        # Remove common PHI patterns (basic anonymization)\n",
        "        text = re.sub(r'\\[\\*\\*.*?\\*\\*\\]', '[REDACTED]', text)  # Remove bracketed PHI\n",
        "\n",
        "        # Normalize whitespace\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "        # Remove control characters\n",
        "        text = ''.join(char for char in text if ord(char) >= 32 or char == '\\n')\n",
        "\n",
        "        return text\n",
        "\n",
        "    def _safe_parse_json(self, json_str: str) -> Optional[List[Dict]]:\n",
        "        \"\"\"Safely parse JSON strings from CSV\"\"\"\n",
        "        if pd.isna(json_str) or json_str == '':\n",
        "            return None\n",
        "\n",
        "        try:\n",
        "            # Handle string representation of list\n",
        "            if isinstance(json_str, str):\n",
        "                # Try ast.literal_eval first\n",
        "                parsed = ast.literal_eval(json_str)\n",
        "                return parsed if isinstance(parsed, list) else None\n",
        "        except (ValueError, SyntaxError):\n",
        "            try:\n",
        "                # Try json.loads as backup\n",
        "                return json.loads(json_str)\n",
        "            except json.JSONDecodeError:\n",
        "                return None\n",
        "        return None\n",
        "\n",
        "    def _extract_codes_as_strings(self, codes_list: Optional[List[Dict]], code_key: str) -> List[str]:\n",
        "        \"\"\"Extract codes from parsed JSON and convert to strings\"\"\"\n",
        "        if not codes_list:\n",
        "            return []\n",
        "\n",
        "        result = []\n",
        "        for item in codes_list:\n",
        "            if isinstance(item, dict) and code_key in item:\n",
        "                code = item[code_key]\n",
        "                if code is not None and str(code).strip():\n",
        "                    result.append(str(code).strip())\n",
        "        return result\n",
        "\n",
        "    def _extract_titles_as_strings(self, codes_list: Optional[List[Dict]], title_key: str) -> List[str]:\n",
        "        \"\"\"Extract titles from parsed JSON and convert to strings\"\"\"\n",
        "        if not codes_list:\n",
        "            return []\n",
        "\n",
        "        result = []\n",
        "        for item in codes_list:\n",
        "            if isinstance(item, dict) and title_key in item:\n",
        "                title = item[title_key]\n",
        "                if title is not None and str(title).strip():\n",
        "                    result.append(str(title).strip())\n",
        "        return result\n",
        "\n",
        "    def chunk_documents(self, df: pd.DataFrame) -> List[Dict[str, Any]]:\n",
        "        \"\"\"Split long medical texts into chunks\"\"\"\n",
        "        chunks = []\n",
        "\n",
        "        for idx, row in df.iterrows():\n",
        "            text = row['TEXT']\n",
        "            if not text or len(text.strip()) < 50:\n",
        "                continue\n",
        "\n",
        "            # Split text into chunks\n",
        "            text_chunks = self.text_splitter.split_text(text)\n",
        "\n",
        "            for chunk_idx, chunk in enumerate(text_chunks):\n",
        "                chunk_data = {\n",
        "                    'content': chunk,\n",
        "                    'original_row_id': int(row['row_id']),\n",
        "                    'subject_id': int(row['SUBJECT_ID']),\n",
        "                    'admission_id': int(row['HADM_ID']),\n",
        "                    'chart_date': row['CHARTDATE'].isoformat() if pd.notna(row['CHARTDATE']) else None,\n",
        "                    'category': str(row['CATEGORY']),\n",
        "                    'description': str(row['DESCRIPTION']),\n",
        "                    'chunk_id': chunk_idx,\n",
        "                    'total_chunks': len(text_chunks),\n",
        "\n",
        "                    # Medical codes\n",
        "                    'diagnoses': row['diagnoses_parsed'] or [],\n",
        "                    'procedures': row['procedures_parsed'] or [],\n",
        "                    'cpt_codes': row['cpt_codes_parsed'] or [],\n",
        "\n",
        "                    # Extract code strings properly\n",
        "                    'diagnosis_codes': self._extract_codes_as_strings(row['diagnoses_parsed'], 'ICD9_CODE'),\n",
        "                    'diagnosis_titles': self._extract_titles_as_strings(row['diagnoses_parsed'], 'LONG_TITLE'),\n",
        "                    'procedure_codes': self._extract_codes_as_strings(row['procedures_parsed'], 'ICD9_CODE'),\n",
        "                    'procedure_titles': self._extract_titles_as_strings(row['procedures_parsed'], 'LONG_TITLE'),\n",
        "                    'cpt_code_values': self._extract_codes_as_strings(row['cpt_codes_parsed'], 'CPT_CD'),\n",
        "                    'cpt_descriptions': self._extract_titles_as_strings(row['cpt_codes_parsed'], 'DESCRIPTION')\n",
        "                }\n",
        "                chunks.append(chunk_data)\n",
        "\n",
        "        print(f\"Created {len(chunks)} text chunks\")\n",
        "        return chunks\n",
        "\n",
        "class VertexAIMedicalRAG:\n",
        "    \"\"\"Medical RAG system using Google Cloud Storage and Vertex AI\"\"\"\n",
        "\n",
        "    def __init__(self, project_id: str, location: str, bucket_name: str):\n",
        "        self.project_id = project_id\n",
        "        self.location = location\n",
        "        self.bucket_name = bucket_name\n",
        "\n",
        "        # Initialize Vertex AI\n",
        "        vertexai.init(project=project_id, location=location)\n",
        "\n",
        "        # Initialize models\n",
        "        self.embedding_model = TextEmbeddingModel.from_pretrained(\"text-embedding-004\")\n",
        "        self.generative_model = GenerativeModel(\"gemini-2.5-flash\")\n",
        "\n",
        "        # Initialize GCS client\n",
        "        self.storage_client = storage.Client(project=project_id)\n",
        "        self.bucket = self.storage_client.bucket(bucket_name)\n",
        "\n",
        "        # In-memory storage for embeddings (loaded from GCS)\n",
        "        self.medical_records = []\n",
        "\n",
        "        print(f\"✅ Initialized Vertex AI Medical RAG\")\n",
        "        print(f\"   Project: {project_id}\")\n",
        "        print(f\"   Location: {location}\")\n",
        "        print(f\"   Bucket: {bucket_name}\")\n",
        "\n",
        "    def generate_embeddings(self, texts: List[str], batch_size: int = 32) -> List[List[float]]:\n",
        "        \"\"\"Generate embeddings using Vertex AI in batches\"\"\"\n",
        "        all_embeddings = []\n",
        "\n",
        "        print(f\"Generating embeddings for {len(texts)} texts in batches of {batch_size}...\")\n",
        "\n",
        "        for i in range(0, len(texts), batch_size):\n",
        "            batch = texts[i:i+batch_size]\n",
        "            try:\n",
        "                embeddings = self.embedding_model.get_embeddings(batch)\n",
        "                batch_embeddings = [e.values for e in embeddings]\n",
        "                all_embeddings.extend(batch_embeddings)\n",
        "\n",
        "                print(f\"  Processed batch {i//batch_size + 1}/{(len(texts)-1)//batch_size + 1}\")\n",
        "                time.sleep(0.5)  # Rate limiting\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"❌ Error in batch {i//batch_size + 1}: {e}\")\n",
        "                # Add empty embeddings for failed batch\n",
        "                empty_embeddings = [[0.0] * 768 for _ in batch]  # text-embedding-004 has 768 dimensions\n",
        "                all_embeddings.extend(empty_embeddings)\n",
        "\n",
        "        return all_embeddings\n",
        "\n",
        "    def create_embeddings_and_store(self, chunks: List[Dict[str, Any]]):\n",
        "        \"\"\"Create embeddings for chunks and store in GCS\"\"\"\n",
        "        print(\"=== EMBEDDING CREATION PHASE ===\")\n",
        "\n",
        "        # Extract texts for embedding\n",
        "        texts = [chunk['content'] for chunk in chunks]\n",
        "\n",
        "        # Generate embeddings\n",
        "        embeddings = self.generate_embeddings(texts)\n",
        "\n",
        "        # Combine chunks with embeddings\n",
        "        medical_records = []\n",
        "        for chunk, embedding in zip(chunks, embeddings):\n",
        "            record = {\n",
        "                **chunk,  # Include all chunk data\n",
        "                'embedding': embedding,\n",
        "                'embedding_id': f\"{chunk['original_row_id']}_{chunk['chunk_id']}\"\n",
        "            }\n",
        "            medical_records.append(record)\n",
        "\n",
        "        # Save to local JSONL first\n",
        "        os.makedirs(\"embeddings\", exist_ok=True)\n",
        "        local_path = \"embeddings/medical_embeddings.jsonl\"\n",
        "\n",
        "        with open(local_path, \"w\") as f:\n",
        "            for record in medical_records:\n",
        "                f.write(json.dumps(record, default=str) + \"\\n\")\n",
        "\n",
        "        print(f\"✅ Saved {len(medical_records)} records to {local_path}\")\n",
        "\n",
        "        # Upload to GCS\n",
        "        gcs_path = \"embeddings/medical_embeddings.jsonl\"\n",
        "        blob = self.bucket.blob(gcs_path)\n",
        "        blob.upload_from_filename(local_path)\n",
        "\n",
        "        print(f\"⬆️ Uploaded to gs://{self.bucket_name}/{gcs_path}\")\n",
        "\n",
        "        # Store in memory for immediate use\n",
        "        self.medical_records = medical_records\n",
        "        print(f\"✅ Loaded {len(self.medical_records)} records into memory\")\n",
        "\n",
        "    def load_embeddings_from_gcs(self):\n",
        "        \"\"\"Load embeddings from GCS into memory\"\"\"\n",
        "        try:\n",
        "            gcs_path = \"embeddings/medical_embeddings.jsonl\"\n",
        "            blob = self.bucket.blob(gcs_path)\n",
        "\n",
        "            # Download to local file\n",
        "            local_path = \"/tmp/medical_embeddings.jsonl\"\n",
        "            blob.download_to_filename(local_path)\n",
        "\n",
        "            # Load into memory\n",
        "            self.medical_records = []\n",
        "            with open(local_path, \"r\") as f:\n",
        "                for line in f:\n",
        "                    record = json.loads(line.strip())\n",
        "                    self.medical_records.append(record)\n",
        "\n",
        "            print(f\"✅ Loaded {len(self.medical_records)} records from GCS\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error loading from GCS: {e}\")\n",
        "            print(\"Please run embedding creation first.\")\n",
        "\n",
        "    def search_similar_records(self, query: str, top_k: int = 10) -> List[Dict[str, Any]]:\n",
        "        \"\"\"Search for similar medical records using cosine similarity\"\"\"\n",
        "        if not self.medical_records:\n",
        "            print(\"❌ No medical records loaded. Please load embeddings first.\")\n",
        "            return []\n",
        "\n",
        "        print(f\"🔍 Searching for: '{query}'\")\n",
        "\n",
        "        # Generate query embedding\n",
        "        query_embeddings = self.embedding_model.get_embeddings([query])\n",
        "        query_vector = np.array([query_embeddings[0].values])\n",
        "\n",
        "        # Get all record embeddings\n",
        "        record_embeddings = np.array([record['embedding'] for record in self.medical_records])\n",
        "\n",
        "        # Calculate similarities\n",
        "        similarities = cosine_similarity(query_vector, record_embeddings)[0]\n",
        "\n",
        "        # Get top-k similar records\n",
        "        top_indices = np.argsort(similarities)[::-1][:top_k]\n",
        "\n",
        "        similar_records = []\n",
        "        for idx in top_indices:\n",
        "            record = self.medical_records[idx].copy()\n",
        "            record['similarity_score'] = float(similarities[idx])\n",
        "            similar_records.append(record)\n",
        "\n",
        "        return similar_records\n",
        "\n",
        "    def extract_medical_codes_from_results(self, similar_records: List[Dict[str, Any]]) -> Dict[str, Dict]:\n",
        "        \"\"\"Extract and aggregate medical codes from search results\"\"\"\n",
        "        medical_codes = {\n",
        "            'diagnoses': {},\n",
        "            'procedures': {},\n",
        "            'cpt_codes': {}\n",
        "        }\n",
        "\n",
        "        for record in similar_records:\n",
        "            # Process diagnoses\n",
        "            for diag in record.get('diagnoses', []):\n",
        "                code = diag.get('ICD9_CODE')\n",
        "                if code:\n",
        "                    if code not in medical_codes['diagnoses']:\n",
        "                        medical_codes['diagnoses'][code] = {\n",
        "                            'short_title': diag.get('SHORT_TITLE', ''),\n",
        "                            'long_title': diag.get('LONG_TITLE', ''),\n",
        "                            'frequency': 0,\n",
        "                            'max_similarity': 0\n",
        "                        }\n",
        "                    medical_codes['diagnoses'][code]['frequency'] += 1\n",
        "                    medical_codes['diagnoses'][code]['max_similarity'] = max(\n",
        "                        medical_codes['diagnoses'][code]['max_similarity'],\n",
        "                        record['similarity_score']\n",
        "                    )\n",
        "\n",
        "            # Process procedures\n",
        "            for proc in record.get('procedures', []):\n",
        "                code = str(proc.get('ICD9_CODE'))\n",
        "                if code and code != 'None':\n",
        "                    if code not in medical_codes['procedures']:\n",
        "                        medical_codes['procedures'][code] = {\n",
        "                            'short_title': proc.get('SHORT_TITLE', ''),\n",
        "                            'long_title': proc.get('LONG_TITLE', ''),\n",
        "                            'frequency': 0,\n",
        "                            'max_similarity': 0\n",
        "                        }\n",
        "                    medical_codes['procedures'][code]['frequency'] += 1\n",
        "                    medical_codes['procedures'][code]['max_similarity'] = max(\n",
        "                        medical_codes['procedures'][code]['max_similarity'],\n",
        "                        record['similarity_score']\n",
        "                    )\n",
        "\n",
        "            # Process CPT codes\n",
        "            for cpt in record.get('cpt_codes', []):\n",
        "                code = cpt.get('CPT_CD')\n",
        "                if code:\n",
        "                    if code not in medical_codes['cpt_codes']:\n",
        "                        medical_codes['cpt_codes'][code] = {\n",
        "                            'description': cpt.get('DESCRIPTION', ''),\n",
        "                            'section': cpt.get('SECTIONHEADER', ''),\n",
        "                            'subsection': cpt.get('SUBSECTIONHEADER', ''),\n",
        "                            'frequency': 0,\n",
        "                            'max_similarity': 0\n",
        "                        }\n",
        "                    medical_codes['cpt_codes'][code]['frequency'] += 1\n",
        "                    medical_codes['cpt_codes'][code]['max_similarity'] = max(\n",
        "                        medical_codes['cpt_codes'][code]['max_similarity'],\n",
        "                        record['similarity_score']\n",
        "                    )\n",
        "\n",
        "        return medical_codes\n",
        "\n",
        "    def generate_medical_response(self, query: str, similar_records: List[Dict[str, Any]], medical_codes: Dict[str, Dict]) -> str:\n",
        "        \"\"\"Generate explanation using Vertex AI Gemini\"\"\"\n",
        "\n",
        "        # Prepare context from top records\n",
        "        context = \"\\n\".join([\n",
        "            f\"Record {i+1}: {record['content'][:300]}...\"\n",
        "            for i, record in enumerate(similar_records[:3])\n",
        "        ])\n",
        "\n",
        "        # Format medical codes\n",
        "        diagnoses_text = \"\"\n",
        "        if medical_codes['diagnoses']:\n",
        "            diagnoses_text = \"DIAGNOSIS CODES FOUND:\\n\"\n",
        "            for code, info in sorted(medical_codes['diagnoses'].items(),\n",
        "                                   key=lambda x: x[1]['max_similarity'], reverse=True)[:5]:\n",
        "                diagnoses_text += f\"- {code}: {info['long_title']} (frequency: {info['frequency']}, similarity: {info['max_similarity']:.3f})\\n\"\n",
        "\n",
        "        procedures_text = \"\"\n",
        "        if medical_codes['procedures']:\n",
        "            procedures_text = \"PROCEDURE CODES FOUND:\\n\"\n",
        "            for code, info in sorted(medical_codes['procedures'].items(),\n",
        "                                   key=lambda x: x[1]['max_similarity'], reverse=True)[:5]:\n",
        "                procedures_text += f\"- {code}: {info['long_title']} (frequency: {info['frequency']}, similarity: {info['max_similarity']:.3f})\\n\"\n",
        "\n",
        "        cpt_text = \"\"\n",
        "        if medical_codes['cpt_codes']:\n",
        "            cpt_text = \"CPT CODES FOUND:\\n\"\n",
        "            for code, info in sorted(medical_codes['cpt_codes'].items(),\n",
        "                                   key=lambda x: x[1]['max_similarity'], reverse=True)[:5]:\n",
        "                cpt_text += f\"- {code}: {info['description']} (frequency: {info['frequency']}, similarity: {info['max_similarity']:.3f})\\n\"\n",
        "\n",
        "        prompt = f\"\"\"\n",
        "You are a medical coding assistant. Based on the following medical records context and the user's query about diseases/symptoms, provide relevant medical codes and explanations.\n",
        "\n",
        "CONTEXT FROM MEDICAL RECORDS:\n",
        "{context}\n",
        "\n",
        "{diagnoses_text}\n",
        "\n",
        "{procedures_text}\n",
        "\n",
        "{cpt_text}\n",
        "\n",
        "USER QUERY: {query}\n",
        "\n",
        "Please provide a comprehensive response that:\n",
        "1. Explains what medical codes are most relevant to the query\n",
        "2. Provides context on why these codes match the query\n",
        "3. Lists the specific ICD9 diagnosis codes, procedure codes, and CPT codes\n",
        "4. Explains what each code represents in simple terms\n",
        "5. Notes the similarity scores to show relevance\n",
        "\n",
        "Keep your response concise and well-formatted.\n",
        "\"\"\"\n",
        "\n",
        "        try:\n",
        "            response = self.generative_model.generate_content(prompt)\n",
        "            return response.text\n",
        "        except Exception as e:\n",
        "            return f\"Error generating explanation: {e}. However, medical codes were found successfully.\"\n",
        "\n",
        "    def query(self, user_query: str, top_k: int = 10) -> Dict[str, Any]:\n",
        "        \"\"\"Main query interface\"\"\"\n",
        "        print(f\"Processing query: {user_query}\")\n",
        "\n",
        "        # Search for similar records\n",
        "        similar_records = self.search_similar_records(user_query, top_k)\n",
        "\n",
        "        if not similar_records:\n",
        "            return {\n",
        "                'query': user_query,\n",
        "                'explanation': \"No similar medical records found.\",\n",
        "                'medical_codes': {'diagnoses': {}, 'procedures': {}, 'cpt_codes': {}},\n",
        "                'retrieved_records_count': 0,\n",
        "                'sample_records': []\n",
        "            }\n",
        "\n",
        "        # Extract medical codes\n",
        "        medical_codes = self.extract_medical_codes_from_results(similar_records)\n",
        "\n",
        "        # Generate explanation\n",
        "        explanation = self.generate_medical_response(user_query, similar_records, medical_codes)\n",
        "\n",
        "        # Prepare sample records\n",
        "        sample_records = []\n",
        "        for record in similar_records[:3]:\n",
        "            sample_records.append({\n",
        "                'content': record['content'][:200] + \"...\" if len(record['content']) > 200 else record['content'],\n",
        "                'category': record['category'],\n",
        "                'description': record['description'],\n",
        "                'similarity_score': record['similarity_score'],\n",
        "                'row_id': record['original_row_id']\n",
        "            })\n",
        "\n",
        "        return {\n",
        "            'query': user_query,\n",
        "            'explanation': explanation,\n",
        "            'medical_codes': medical_codes,\n",
        "            'retrieved_records_count': len(similar_records),\n",
        "            'sample_records': sample_records\n",
        "        }"
      ],
      "metadata": {
        "id": "hWePOQSXwTVa"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def main():\n",
        "    \"\"\"Main execution function\"\"\"\n",
        "\n",
        "    # Configuration\n",
        "    CSV_PATH = \"/content/ehr_records.csv\"  # Update with your CSV path\n",
        "\n",
        "    # Initialize components\n",
        "    processor = MedicalDataProcessor()\n",
        "    rag_system = VertexAIMedicalRAG(PROJECT_ID, LOCATION, BUCKET_NAME)\n",
        "\n",
        "    # Setup mode\n",
        "    setup_mode = input(\"Do you want to setup the database? (y/n): \").lower() == 'y'\n",
        "\n",
        "    if setup_mode:\n",
        "        print(\"=== DATA PROCESSING PHASE ===\")\n",
        "        df = processor.clean_csv_data(CSV_PATH)\n",
        "        chunks = processor.chunk_documents(df)\n",
        "\n",
        "        print(\"=== EMBEDDING & STORAGE PHASE ===\")\n",
        "        rag_system.create_embeddings_and_store(chunks)\n",
        "        print(\"Setup complete!\")\n",
        "    else:\n",
        "        # Load existing embeddings\n",
        "        print(\"=== LOADING EMBEDDINGS FROM GCS ===\")\n",
        "        rag_system.load_embeddings_from_gcs()\n",
        "\n",
        "    # Interactive query mode\n",
        "    print(\"\\n=== MEDICAL RAG SYSTEM READY ===\")\n",
        "    print(\"Enter medical queries to find relevant codes. Type 'quit' to exit.\\n\")\n",
        "\n",
        "    # Sample queries\n",
        "    sample_queries = [\n",
        "        \"chest pain and shortness of breath\",\n",
        "        \"cardiac arrest and heart failure\",\n",
        "        \"difficulty breathing\",\n",
        "        \"hypotension and shock\",\n",
        "        \"mechanical ventilation\"\n",
        "    ]\n",
        "\n",
        "    print(\"Sample queries you can try:\")\n",
        "    for i, q in enumerate(sample_queries, 1):\n",
        "        print(f\"  {i}. {q}\")\n",
        "    print()\n",
        "\n",
        "    while True:\n",
        "        query = input(\"Enter your medical query (or 'quit' to exit): \").strip()\n",
        "        if query.lower() in ['quit', 'exit', 'q']:\n",
        "            break\n",
        "\n",
        "        if not query:\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            results = rag_system.query(query)\n",
        "\n",
        "            print(\"\\n\" + \"=\"*60)\n",
        "            print(f\"QUERY: {results['query']}\")\n",
        "            print(\"=\"*60)\n",
        "\n",
        "            print(\"🏥 MEDICAL CODES FOUND:\")\n",
        "\n",
        "            # Display diagnoses\n",
        "            if results['medical_codes']['diagnoses']:\n",
        "                print(\"\\n📋 DIAGNOSIS CODES (ICD9):\")\n",
        "                for code, info in results['medical_codes']['diagnoses'].items():\n",
        "                    print(f\"  • {code}: {info['long_title']} (freq: {info['frequency']}, sim: {info.get('max_similarity', 0):.3f})\")\n",
        "\n",
        "            # Display procedures\n",
        "            if results['medical_codes']['procedures']:\n",
        "                print(\"\\n🔬 PROCEDURE CODES (ICD9):\")\n",
        "                for code, info in results['medical_codes']['procedures'].items():\n",
        "                    print(f\"  • {code}: {info['long_title']} (freq: {info['frequency']}, sim: {info.get('max_similarity', 0):.3f})\")\n",
        "\n",
        "            # Display CPT codes\n",
        "            if results['medical_codes']['cpt_codes']:\n",
        "                print(\"\\n💊 CPT CODES:\")\n",
        "                for code, info in results['medical_codes']['cpt_codes'].items():\n",
        "                    print(f\"  • {code}: {info['description']} (freq: {info['frequency']}, sim: {info.get('max_similarity', 0):.3f})\")\n",
        "\n",
        "            if not any([\n",
        "                results['medical_codes']['diagnoses'],\n",
        "                results['medical_codes']['procedures'],\n",
        "                results['medical_codes']['cpt_codes']\n",
        "            ]):\n",
        "                print(\"  No specific medical codes found for this query.\")\n",
        "\n",
        "            print(f\"\\n📄 EXPLANATION:\")\n",
        "            print(results['explanation'])\n",
        "\n",
        "            print(f\"\\n📊 Retrieved {results['retrieved_records_count']} relevant medical records\")\n",
        "            print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error processing query: {e}\")\n",
        "            print(\"Please try again with a different query.\\n\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sa1QZSKXwVJ9",
        "outputId": "366bb494-4254-478d-83f6-ee57f9395ea2"
      },
      "execution_count": 16,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Initialized Vertex AI Medical RAG\n",
            "   Project: i-monolith-468706-i9\n",
            "   Location: us-central1\n",
            "   Bucket: bucket_jonah\n",
            "Do you want to setup the database? (y/n): y\n",
            "=== DATA PROCESSING PHASE ===\n",
            "Loading CSV data...\n",
            "Original data shape: (1976, 14)\n",
            "Cleaned data shape: (1945, 17)\n",
            "Created 3710 text chunks\n",
            "=== EMBEDDING & STORAGE PHASE ===\n",
            "=== EMBEDDING CREATION PHASE ===\n",
            "Generating embeddings for 3710 texts in batches of 32...\n",
            "  Processed batch 1/116\n",
            "  Processed batch 2/116\n",
            "  Processed batch 3/116\n",
            "  Processed batch 4/116\n",
            "  Processed batch 5/116\n",
            "  Processed batch 6/116\n",
            "  Processed batch 7/116\n",
            "  Processed batch 8/116\n",
            "  Processed batch 9/116\n",
            "  Processed batch 10/116\n",
            "  Processed batch 11/116\n",
            "  Processed batch 12/116\n",
            "  Processed batch 13/116\n",
            "  Processed batch 14/116\n",
            "  Processed batch 15/116\n",
            "  Processed batch 16/116\n",
            "  Processed batch 17/116\n",
            "  Processed batch 18/116\n",
            "  Processed batch 19/116\n",
            "  Processed batch 20/116\n",
            "  Processed batch 21/116\n",
            "  Processed batch 22/116\n",
            "  Processed batch 23/116\n",
            "  Processed batch 24/116\n",
            "  Processed batch 25/116\n",
            "  Processed batch 26/116\n",
            "  Processed batch 27/116\n",
            "  Processed batch 28/116\n",
            "  Processed batch 29/116\n",
            "  Processed batch 30/116\n",
            "  Processed batch 31/116\n",
            "  Processed batch 32/116\n",
            "  Processed batch 33/116\n",
            "  Processed batch 34/116\n",
            "  Processed batch 35/116\n",
            "  Processed batch 36/116\n",
            "  Processed batch 37/116\n",
            "  Processed batch 38/116\n",
            "  Processed batch 39/116\n",
            "  Processed batch 40/116\n",
            "  Processed batch 41/116\n",
            "  Processed batch 42/116\n",
            "  Processed batch 43/116\n",
            "  Processed batch 44/116\n",
            "  Processed batch 45/116\n",
            "  Processed batch 46/116\n",
            "  Processed batch 47/116\n",
            "  Processed batch 48/116\n",
            "  Processed batch 49/116\n",
            "  Processed batch 50/116\n",
            "  Processed batch 51/116\n",
            "  Processed batch 52/116\n",
            "  Processed batch 53/116\n",
            "  Processed batch 54/116\n",
            "  Processed batch 55/116\n",
            "  Processed batch 56/116\n",
            "  Processed batch 57/116\n",
            "  Processed batch 58/116\n",
            "  Processed batch 59/116\n",
            "  Processed batch 60/116\n",
            "  Processed batch 61/116\n",
            "  Processed batch 62/116\n",
            "  Processed batch 63/116\n",
            "  Processed batch 64/116\n",
            "  Processed batch 65/116\n",
            "  Processed batch 66/116\n",
            "  Processed batch 67/116\n",
            "  Processed batch 68/116\n",
            "  Processed batch 69/116\n",
            "  Processed batch 70/116\n",
            "  Processed batch 71/116\n",
            "  Processed batch 72/116\n",
            "  Processed batch 73/116\n",
            "  Processed batch 74/116\n",
            "  Processed batch 75/116\n",
            "  Processed batch 76/116\n",
            "  Processed batch 77/116\n",
            "  Processed batch 78/116\n",
            "  Processed batch 79/116\n",
            "  Processed batch 80/116\n",
            "  Processed batch 81/116\n",
            "  Processed batch 82/116\n",
            "  Processed batch 83/116\n",
            "  Processed batch 84/116\n",
            "  Processed batch 85/116\n",
            "  Processed batch 86/116\n",
            "  Processed batch 87/116\n",
            "  Processed batch 88/116\n",
            "  Processed batch 89/116\n",
            "  Processed batch 90/116\n",
            "  Processed batch 91/116\n",
            "  Processed batch 92/116\n",
            "  Processed batch 93/116\n",
            "  Processed batch 94/116\n",
            "  Processed batch 95/116\n",
            "  Processed batch 96/116\n",
            "  Processed batch 97/116\n",
            "  Processed batch 98/116\n",
            "  Processed batch 99/116\n",
            "  Processed batch 100/116\n",
            "  Processed batch 101/116\n",
            "  Processed batch 102/116\n",
            "  Processed batch 103/116\n",
            "  Processed batch 104/116\n",
            "  Processed batch 105/116\n",
            "  Processed batch 106/116\n",
            "  Processed batch 107/116\n",
            "  Processed batch 108/116\n",
            "  Processed batch 109/116\n",
            "  Processed batch 110/116\n",
            "  Processed batch 111/116\n",
            "  Processed batch 112/116\n",
            "  Processed batch 113/116\n",
            "  Processed batch 114/116\n",
            "  Processed batch 115/116\n",
            "  Processed batch 116/116\n",
            "✅ Saved 3710 records to embeddings/medical_embeddings.jsonl\n",
            "⬆️ Uploaded to gs://bucket_jonah/embeddings/medical_embeddings.jsonl\n",
            "✅ Loaded 3710 records into memory\n",
            "Setup complete!\n",
            "\n",
            "=== MEDICAL RAG SYSTEM READY ===\n",
            "Enter medical queries to find relevant codes. Type 'quit' to exit.\n",
            "\n",
            "Sample queries you can try:\n",
            "  1. chest pain and shortness of breath\n",
            "  2. cardiac arrest and heart failure\n",
            "  3. difficulty breathing\n",
            "  4. hypotension and shock\n",
            "  5. mechanical ventilation\n",
            "\n",
            "Enter your medical query (or 'quit' to exit): chest pain and shortness of breath\n",
            "Processing query: chest pain and shortness of breath\n",
            "🔍 Searching for: 'chest pain and shortness of breath'\n",
            "\n",
            "============================================================\n",
            "QUERY: chest pain and shortness of breath\n",
            "============================================================\n",
            "🏥 MEDICAL CODES FOUND:\n",
            "\n",
            "📋 DIAGNOSIS CODES (ICD9):\n",
            "  • 99681: Complications of transplanted kidney (freq: 1, sim: 0.591)\n",
            "  • 42833: Acute on chronic diastolic heart failure (freq: 1, sim: 0.591)\n",
            "  • 41071: Subendocardial infarction, initial episode of care (freq: 1, sim: 0.591)\n",
            "  • 40311: Hypertensive chronic kidney disease, benign, with chronic kidney disease stage V or end stage renal disease (freq: 1, sim: 0.591)\n",
            "  • 25200: Hyperparathyroidism, unspecified (freq: 1, sim: 0.591)\n",
            "  • 5856: End stage renal disease (freq: 1, sim: 0.591)\n",
            "  • 2749: Gout, unspecified (freq: 1, sim: 0.591)\n",
            "  • 27800: Obesity, unspecified (freq: 1, sim: 0.591)\n",
            "  • 2811: Other vitamin B12 deficiency anemia (freq: 1, sim: 0.591)\n",
            "  • 4280: Congestive heart failure, unspecified (freq: 1, sim: 0.591)\n",
            "  • E8780: Surgical operation with transplant of whole organ causing abnormal patient reaction, or later complication, without mention of misadventure at time of operation (freq: 1, sim: 0.591)\n",
            "  • 41401: Coronary atherosclerosis of native coronary artery (freq: 1, sim: 0.591)\n",
            "  • 28521: Anemia in chronic kidney disease (freq: 1, sim: 0.591)\n",
            "  • 2767: Hyperpotassemia (freq: 1, sim: 0.591)\n",
            "  • V1005: Personal history of malignant neoplasm of large intestine (freq: 1, sim: 0.591)\n",
            "\n",
            "🔬 PROCEDURE CODES (ICD9):\n",
            "  • 9390: Non-invasive mechanical ventilation (freq: 9, sim: 0.653)\n",
            "  • 9904: Transfusion of packed cells (freq: 9, sim: 0.653)\n",
            "  • 3995: Hemodialysis (freq: 10, sim: 0.653)\n",
            "  • 8867: Phlebography of other specified sites using contrast material (freq: 2, sim: 0.608)\n",
            "  • 5491: Percutaneous abdominal drainage (freq: 2, sim: 0.608)\n",
            "\n",
            "📄 EXPLANATION:\n",
            "Based on the medical records and your query about \"chest pain and shortness of breath,\" here are the most relevant medical codes:\n",
            "\n",
            "### Relevant Medical Codes and Explanations\n",
            "\n",
            "The patient's records explicitly mention \"chest discomfort\" and \"SOB\" (Shortness Of Breath) multiple times, as well as \"dyspnea\" (medical term for SOB). These symptoms are serious and often indicate cardiac or pulmonary issues, which are reflected in the identified codes.\n",
            "\n",
            "**ICD9 Diagnosis Codes:**\n",
            "\n",
            "1.  **Code:** `41071`\n",
            "    *   **Explanation:** This code represents a **Subendocardial infarction, initial episode of care**. A subendocardial infarction is a type of heart attack, which is a classic cause of chest pain and often accompanied by shortness of breath. Given the patient's existing conditions like malignant HTN and ESRD, cardiac events are a significant concern.\n",
            "    *   **Similarity:** 0.591\n",
            "\n",
            "2.  **Code:** `42833`\n",
            "    *   **Explanation:** This code represents **Acute on chronic diastolic heart failure**. Heart failure is a common cause of shortness of breath (especially with exertion or at rest) and can also lead to chest discomfort. The patient's ESRD and hypertension predispose her to heart failure.\n",
            "    *   **Similarity:** 0.591\n",
            "\n",
            "**ICD9 Procedure Codes:**\n",
            "\n",
            "1.  **Code:** `9390`\n",
            "    *   **Explanation:** This code represents **Non-invasive mechanical ventilation**. This procedure is directly used to assist breathing for patients experiencing severe shortness of breath or respiratory distress. Its high frequency (9) suggests it's a commonly associated intervention in this patient's records, likely due to respiratory issues like SOB.\n",
            "    *   **Similarity:** 0.653\n",
            "\n",
            "**Contextual Relevance:**\n",
            "\n",
            "*   The patient presents with severe symptoms like \"HTN to 230s,\" \"central crampy abdominal pain,\" \"chest discomfort,\" and \"SOB,\" suggesting an acute and serious medical situation.\n",
            "*   The diagnosis codes `41071` (heart attack) and `42833` (heart failure) directly address severe underlying conditions that manifest with chest pain/discomfort and shortness of breath.\n",
            "*   The procedure code `9390` (non-invasive mechanical ventilation) is a direct intervention for respiratory distress, which aligns perfectly with \"shortness of breath.\"\n",
            "*   While `40311` (Hypertensive CKD with ESRD) is a key diagnosis for the patient, it describes the underlying chronic condition rather than the acute symptoms of chest pain and SOB directly. Similarly, `3995` (Hemodialysis) is for her ESRD management, though fluid overload related to ESRD can cause SOB. However, the selected codes are more directly indicative of the acute presentation of chest pain and SOB.\n",
            "\n",
            "**CPT Codes:**\n",
            "No specific CPT codes were provided in the \"PROCEDURE CODES FOUND\" list; the procedures listed are ICD9 procedure codes.\n",
            "\n",
            "📊 Retrieved 10 relevant medical records\n",
            "============================================================\n",
            "\n",
            "Enter your medical query (or 'quit' to exit): quit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "!pip install streamlit -q\n",
        "!pip install google-generativeai -q\n",
        "!pip install PyPDF2 -q\n",
        "!pip install python-docx -q\n",
        "!pip install SpeechRecognition -q\n",
        "!pip install gTTS -q\n",
        "!pip install Pillow -q\n",
        "!pip install pyngrok -q\n",
        "\n",
        "# Install system dependencies for audio processing\n",
        "!apt-get update\n",
        "!apt-get install -y portaudio19-dev python3-pyaudio\n",
        "!pip install pyaudio\n",
        "\n",
        "# Install additional audio processing libraries\n",
        "!apt-get install -y ffmpeg\n",
        "!pip install pydub -q\n",
        "\n",
        "!apt-get install ffmpeg\n",
        "!pip install --quiet streamlit streamlit-audiorecorder git+https://github.com/stlukey/whispercpp.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5og16J9TMjt7",
        "outputId": "201d3160-a398-4b99-fac1-470ee0a145ea"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m51.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m68.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.0/253.0 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m32.9/32.9 MB\u001b[0m \u001b[31m47.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.2/98.2 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-adk 1.13.0 requires google-cloud-aiplatform[agent-engines]<2.0.0,>=1.95.1, but you have google-cloud-aiplatform 1.71.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "Get:1 https://cli.github.com/packages stable InRelease [3,917 B]\n",
            "Get:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Get:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:4 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:5 https://cli.github.com/packages stable/main amd64 Packages [346 B]\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:7 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:9 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [2,006 kB]\n",
            "Hit:10 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:11 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,797 kB]\n",
            "Get:12 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease [24.3 kB]\n",
            "Get:13 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,267 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Hit:15 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:16 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,310 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,581 kB]\n",
            "Get:18 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,274 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu jammy-updates/multiverse amd64 Packages [88.8 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [5,627 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,623 kB]\n",
            "Fetched 30.0 MB in 3s (10.1 MB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libportaudio2 libportaudiocpp0\n",
            "Suggested packages:\n",
            "  portaudio19-doc python-pyaudio-doc\n",
            "The following NEW packages will be installed:\n",
            "  libportaudio2 libportaudiocpp0 portaudio19-dev python3-pyaudio\n",
            "0 upgraded, 4 newly installed, 0 to remove and 49 not upgraded.\n",
            "Need to get 213 kB of archives.\n",
            "After this operation, 1,043 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libportaudio2 amd64 19.6.0-1.1 [65.3 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libportaudiocpp0 amd64 19.6.0-1.1 [16.1 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 portaudio19-dev amd64 19.6.0-1.1 [106 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/universe amd64 python3-pyaudio amd64 0.2.11-1.3ubuntu1 [25.9 kB]\n",
            "Fetched 213 kB in 1s (265 kB/s)\n",
            "Selecting previously unselected package libportaudio2:amd64.\n",
            "(Reading database ... 126374 files and directories currently installed.)\n",
            "Preparing to unpack .../libportaudio2_19.6.0-1.1_amd64.deb ...\n",
            "Unpacking libportaudio2:amd64 (19.6.0-1.1) ...\n",
            "Selecting previously unselected package libportaudiocpp0:amd64.\n",
            "Preparing to unpack .../libportaudiocpp0_19.6.0-1.1_amd64.deb ...\n",
            "Unpacking libportaudiocpp0:amd64 (19.6.0-1.1) ...\n",
            "Selecting previously unselected package portaudio19-dev:amd64.\n",
            "Preparing to unpack .../portaudio19-dev_19.6.0-1.1_amd64.deb ...\n",
            "Unpacking portaudio19-dev:amd64 (19.6.0-1.1) ...\n",
            "Selecting previously unselected package python3-pyaudio.\n",
            "Preparing to unpack .../python3-pyaudio_0.2.11-1.3ubuntu1_amd64.deb ...\n",
            "Unpacking python3-pyaudio (0.2.11-1.3ubuntu1) ...\n",
            "Setting up libportaudio2:amd64 (19.6.0-1.1) ...\n",
            "Setting up libportaudiocpp0:amd64 (19.6.0-1.1) ...\n",
            "Setting up portaudio19-dev:amd64 (19.6.0-1.1) ...\n",
            "Setting up python3-pyaudio (0.2.11-1.3ubuntu1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero_v2.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "Requirement already satisfied: pyaudio in /usr/lib/python3/dist-packages (0.2.11)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 49 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 49 not upgraded.\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m487.7/487.7 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for whispercpp (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langfuse langgraph -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fhS2L_M5W2On",
        "outputId": "51e52cd1-ba6f-4640-9813-502de8c34a67"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.3/153.3 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.9/43.9 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.7/216.7 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import os\n",
        "import tempfile\n",
        "import traceback\n",
        "import io\n",
        "import base64\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from typing import List, Dict, Any, Optional, TypedDict\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import time\n",
        "import uuid\n",
        "import re\n",
        "\n",
        "# Langfuse import\n",
        "from langfuse import Langfuse\n",
        "\n",
        "# Vertex AI imports\n",
        "import vertexai\n",
        "from vertexai.preview.language_models import TextEmbeddingModel\n",
        "from vertexai.generative_models import GenerativeModel\n",
        "from google.cloud import storage\n",
        "\n",
        "# LangGraph imports\n",
        "try:\n",
        "    from langgraph.graph import StateGraph, END\n",
        "    from langgraph.prebuilt import ToolNode\n",
        "    from langgraph.checkpoint.memory import MemorySaver\n",
        "    LANGGRAPH_ENABLED = True\n",
        "except ImportError:\n",
        "    LANGGRAPH_ENABLED = False\n",
        "    st.warning(\"LangGraph not available. Install langgraph for advanced workflow features.\")\n",
        "\n",
        "# Audio and file processing imports\n",
        "try:\n",
        "    import speech_recognition as sr\n",
        "    from gtts import gTTS\n",
        "    import PyPDF2\n",
        "    import docx\n",
        "    from PIL import Image\n",
        "    from tempfile import NamedTemporaryFile\n",
        "    from audiorecorder import audiorecorder\n",
        "    from whispercpp import Whisper\n",
        "    AUDIO_ENABLED = True\n",
        "except ImportError:\n",
        "    AUDIO_ENABLED = False\n",
        "    st.warning(\"Some audio features disabled. Install required packages for full functionality.\")\n",
        "\n",
        "# Page configuration\n",
        "st.set_page_config(\n",
        "    page_title=\"TechCare Solutions Chatbot\",\n",
        "    page_icon=\"🏥\",\n",
        "    layout=\"wide\",\n",
        "    initial_sidebar_state=\"expanded\"\n",
        ")\n",
        "\n",
        "# Enhanced CSS for fixed input interface\n",
        "st.markdown(\"\"\"\n",
        "<style>\n",
        "    /* Clean, minimal styling */\n",
        "    .stApp {\n",
        "        background-color: #ffffff;\n",
        "        color: #333333;\n",
        "    }\n",
        "\n",
        "    /* Fixed input container */\n",
        "    .fixed-input-container {\n",
        "        position: fixed;\n",
        "        bottom: 0;\n",
        "        left: 0;\n",
        "        right: 0;\n",
        "        background: white;\n",
        "        padding: 15px;\n",
        "        border-top: 2px solid #f0f0f0;\n",
        "        z-index: 1000;\n",
        "        box-shadow: 0 -2px 10px rgba(0,0,0,0.1);\n",
        "    }\n",
        "\n",
        "    /* Adjust main content to account for fixed input */\n",
        "    .main-content {\n",
        "        margin-bottom: 120px;\n",
        "        padding-bottom: 20px;\n",
        "    }\n",
        "\n",
        "    /* Medical code styling */\n",
        "    .medical-code-box {\n",
        "        background: #f8f9fa;\n",
        "        border-left: 4px solid #4CAF50;\n",
        "        padding: 15px;\n",
        "        margin: 15px 0;\n",
        "        border-radius: 8px;\n",
        "        border: 1px solid #e9ecef;\n",
        "        font-family: \"Courier New\", monospace;\n",
        "    }\n",
        "\n",
        "    .code-highlight {\n",
        "        background: #e9ecef;\n",
        "        color: #006600;\n",
        "        padding: 2px 6px;\n",
        "        border-radius: 4px;\n",
        "        font-weight: bold;\n",
        "        font-family: \"Courier New\", monospace;\n",
        "    }\n",
        "\n",
        "    /* Voice input feedback */\n",
        "    .voice-input-feedback {\n",
        "        background: #f0f8ff;\n",
        "        border: 2px solid #4CAF50;\n",
        "        border-radius: 8px;\n",
        "        padding: 15px;\n",
        "        margin: 15px 0;\n",
        "        color: #2d5a2d;\n",
        "        font-weight: 500;\n",
        "        animation: pulse 2s infinite;\n",
        "    }\n",
        "\n",
        "    @keyframes pulse {\n",
        "        0% { border-color: #4CAF50; }\n",
        "        50% { border-color: #81C784; }\n",
        "        100% { border-color: #4CAF50; }\n",
        "    }\n",
        "\n",
        "    /* File info styling */\n",
        "    .file-info {\n",
        "        background: #f8f9fa;\n",
        "        border: 1px solid #dee2e6;\n",
        "        border-radius: 8px;\n",
        "        padding: 12px;\n",
        "        margin: 8px 0;\n",
        "        font-size: 0.9em;\n",
        "    }\n",
        "\n",
        "    /* Audio player styling */\n",
        "    .audio-player {\n",
        "        margin: 15px 0;\n",
        "        padding: 15px;\n",
        "        background: #f8f9fa;\n",
        "        border-radius: 8px;\n",
        "        border: 1px solid #dee2e6;\n",
        "    }\n",
        "\n",
        "    .audio-player audio {\n",
        "        width: 100%;\n",
        "        height: 45px;\n",
        "        border-radius: 8px;\n",
        "    }\n",
        "\n",
        "    /* FAQ styling */\n",
        "    .faq-item {\n",
        "        background: #f8f9fa;\n",
        "        border: 1px solid #dee2e6;\n",
        "        border-radius: 6px;\n",
        "        padding: 10px;\n",
        "        margin: 5px 0;\n",
        "        cursor: pointer;\n",
        "        transition: all 0.2s ease;\n",
        "    }\n",
        "\n",
        "    .faq-item:hover {\n",
        "        background: #e9ecef;\n",
        "        border-color: #4CAF50;\n",
        "    }\n",
        "\n",
        "    /* Voice recording indicator */\n",
        "    .recording-indicator {\n",
        "        background: #ffebee;\n",
        "        border: 2px solid #f44336;\n",
        "        border-radius: 8px;\n",
        "        padding: 15px;\n",
        "        text-align: center;\n",
        "        animation: recordingPulse 1s infinite;\n",
        "    }\n",
        "\n",
        "    @keyframes recordingPulse {\n",
        "        0% { background-color: #ffebee; }\n",
        "        50% { background-color: #ffcdd2; }\n",
        "        100% { background-color: #ffebee; }\n",
        "    }\n",
        "\n",
        "    /* Input interface styling */\n",
        "    .input-interface {\n",
        "        display: flex;\n",
        "        gap: 10px;\n",
        "        align-items: flex-end;\n",
        "        background: white;\n",
        "        padding: 10px;\n",
        "        border-radius: 8px;\n",
        "        border: 1px solid #dee2e6;\n",
        "    }\n",
        "\n",
        "    .voice-button {\n",
        "        min-width: 60px;\n",
        "        height: 60px;\n",
        "        border-radius: 50%;\n",
        "        display: flex;\n",
        "        align-items: center;\n",
        "        justify-content: center;\n",
        "        font-size: 20px;\n",
        "    }\n",
        "\n",
        "    /* Workflow status styling */\n",
        "    .workflow-status {\n",
        "        background: #e3f2fd;\n",
        "        border: 1px solid #2196f3;\n",
        "        border-radius: 8px;\n",
        "        padding: 10px;\n",
        "        margin: 10px 0;\n",
        "        font-size: 0.9em;\n",
        "    }\n",
        "</style>\n",
        "\"\"\", unsafe_allow_html=True)\n",
        "\n",
        "# Configuration\n",
        "PROJECT_ID = \"i-monolith-468706-i9\"\n",
        "LOCATION = \"us-central1\"\n",
        "BUCKET_NAME = \"bucket_jonah\"\n",
        "\n",
        "# Langfuse API Keys\n",
        "LANGFUSE_SECRET_KEY = \"sk-lf-fb61a69a-7805-4423-86fe-71d9cf1ed2aa\"\n",
        "LANGFUSE_PUBLIC_KEY = \"pk-lf-7c1d54e4-6bae-40db-ad9b-d469c92eb855\"\n",
        "LANGFUSE_HOST = \"https://cloud.langfuse.com\"\n",
        "\n",
        "# Sample queries for FAQ\n",
        "SAMPLE_QUERIES = [\n",
        "    \"chest pain and shortness of breath\",\n",
        "    \"cardiac arrest and heart failure\",\n",
        "    \"difficulty breathing\",\n",
        "    \"hypotension and shock\",\n",
        "    \"mechanical ventilation\",\n",
        "]\n",
        "\n",
        "# LangGraph State Definition\n",
        "class WorkflowState(TypedDict):\n",
        "    \"\"\"State for the medical RAG workflow\"\"\"\n",
        "    user_query: str\n",
        "    original_query: str\n",
        "    processed_query: str\n",
        "    query_intent: str\n",
        "    confidence_threshold: float\n",
        "    top_k: int\n",
        "    file_context: str\n",
        "    similar_records: List[Dict[str, Any]]\n",
        "    medical_codes: Dict[str, Dict]\n",
        "    rag_response: str\n",
        "    fallback_response: str\n",
        "    final_response: str\n",
        "    use_fallback: bool\n",
        "    fallback_reason: str\n",
        "    max_similarity: float\n",
        "    workflow_status: List[str]\n",
        "    trace_id: Optional[str]\n",
        "    error: Optional[str]\n",
        "\n",
        "@st.cache_resource\n",
        "def initialize_langfuse():\n",
        "    \"\"\"Initialize Langfuse client once and cache it\"\"\"\n",
        "    try:\n",
        "        langfuse = Langfuse(\n",
        "            secret_key=LANGFUSE_SECRET_KEY,\n",
        "            public_key=LANGFUSE_PUBLIC_KEY,\n",
        "            host=LANGFUSE_HOST\n",
        "        )\n",
        "        return langfuse\n",
        "    except Exception as e:\n",
        "        st.error(f\"Failed to initialize Langfuse: {e}\")\n",
        "        return None\n",
        "\n",
        "@st.cache_resource\n",
        "def load_whisper_model():\n",
        "    \"\"\"Load Whisper model once and cache it\"\"\"\n",
        "    if AUDIO_ENABLED:\n",
        "        try:\n",
        "            return Whisper(\"tiny\")\n",
        "        except Exception as e:\n",
        "            st.error(f\"Failed to load Whisper model: {e}\")\n",
        "            return None\n",
        "    return None\n",
        "\n",
        "class QueryAnalysisAgent:\n",
        "    \"\"\"Agent responsible for analyzing and preprocessing user queries\"\"\"\n",
        "\n",
        "    def __init__(self, generative_model):\n",
        "        self.generative_model = generative_model\n",
        "\n",
        "    def analyze_query(self, state: WorkflowState) -> WorkflowState:\n",
        "        \"\"\"Analyze user query intent and preprocess it\"\"\"\n",
        "        try:\n",
        "            analysis_prompt = f\"\"\"\n",
        "            Analyze the following medical query and provide:\n",
        "            1. Query intent (diagnostic, procedural, symptom-based, code-lookup, general)\n",
        "            2. Key medical terms extracted\n",
        "            3. Processed query (cleaned and expanded with medical synonyms)\n",
        "\n",
        "            Original Query: {state['user_query']}\n",
        "\n",
        "            Provide response in JSON format:\n",
        "            {{\n",
        "                \"intent\": \"intent_type\",\n",
        "                \"key_terms\": [\"term1\", \"term2\"],\n",
        "                \"processed_query\": \"enhanced query with medical terms\"\n",
        "            }}\n",
        "            \"\"\"\n",
        "\n",
        "            response = self.generative_model.generate_content(analysis_prompt)\n",
        "            try:\n",
        "                analysis = json.loads(response.text)\n",
        "                state['query_intent'] = analysis.get('intent', 'general')\n",
        "                state['processed_query'] = analysis.get('processed_query', state['user_query'])\n",
        "            except json.JSONDecodeError:\n",
        "                # Fallback if JSON parsing fails\n",
        "                state['query_intent'] = 'general'\n",
        "                state['processed_query'] = state['user_query']\n",
        "\n",
        "            state['workflow_status'].append(f\"✓ Query analyzed - Intent: {state['query_intent']}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            state['error'] = f\"Query analysis error: {str(e)}\"\n",
        "            state['processed_query'] = state['user_query']\n",
        "            state['query_intent'] = 'general'\n",
        "\n",
        "        return state\n",
        "\n",
        "class RetrievalAgent:\n",
        "    \"\"\"Agent responsible for retrieving similar medical records\"\"\"\n",
        "\n",
        "    def __init__(self, rag_system):\n",
        "        self.rag_system = rag_system\n",
        "\n",
        "    def retrieve_records(self, state: WorkflowState) -> WorkflowState:\n",
        "        \"\"\"Retrieve similar medical records based on processed query\"\"\"\n",
        "        try:\n",
        "            # Prepare enhanced query with file context\n",
        "            enhanced_query = state['processed_query']\n",
        "            if state.get('file_context'):\n",
        "                enhanced_query += f\"\\n\\nAdditional context: {state['file_context'][:1000]}\"\n",
        "\n",
        "            # Search for similar records\n",
        "            similar_records = self.rag_system.search_similar_records(enhanced_query, state['top_k'])\n",
        "            state['similar_records'] = similar_records\n",
        "\n",
        "            if similar_records:\n",
        "                state['max_similarity'] = max([record['similarity_score'] for record in similar_records])\n",
        "                state['workflow_status'].append(f\"✓ Retrieved {len(similar_records)} records - Max similarity: {state['max_similarity']:.3f}\")\n",
        "            else:\n",
        "                state['max_similarity'] = 0.0\n",
        "                state['workflow_status'].append(\"✓ No similar records found\")\n",
        "\n",
        "        except Exception as e:\n",
        "            state['error'] = f\"Retrieval error: {str(e)}\"\n",
        "            state['similar_records'] = []\n",
        "            state['max_similarity'] = 0.0\n",
        "\n",
        "        return state\n",
        "\n",
        "class MedicalCodingAgent:\n",
        "    \"\"\"Agent responsible for extracting and processing medical codes\"\"\"\n",
        "\n",
        "    def __init__(self, rag_system):\n",
        "        self.rag_system = rag_system\n",
        "\n",
        "    def extract_codes(self, state: WorkflowState) -> WorkflowState:\n",
        "        \"\"\"Extract medical codes from retrieved records\"\"\"\n",
        "        try:\n",
        "            if state['similar_records']:\n",
        "                medical_codes = self.rag_system.extract_medical_codes_from_results(state['similar_records'])\n",
        "                state['medical_codes'] = medical_codes\n",
        "\n",
        "                total_codes = sum(len(codes) for codes in medical_codes.values())\n",
        "                state['workflow_status'].append(f\"✓ Extracted {total_codes} medical codes\")\n",
        "            else:\n",
        "                state['medical_codes'] = {'diagnoses': {}, 'procedures': {}, 'cpt_codes': {}}\n",
        "                state['workflow_status'].append(\"✓ No medical codes to extract\")\n",
        "\n",
        "        except Exception as e:\n",
        "            state['error'] = f\"Medical coding error: {str(e)}\"\n",
        "            state['medical_codes'] = {'diagnoses': {}, 'procedures': {}, 'cpt_codes': {}}\n",
        "\n",
        "        return state\n",
        "\n",
        "class ResponseGenerationAgent:\n",
        "    \"\"\"Agent responsible for generating contextual responses\"\"\"\n",
        "\n",
        "    def __init__(self, rag_system):\n",
        "        self.rag_system = rag_system\n",
        "\n",
        "    def generate_response(self, state: WorkflowState) -> WorkflowState:\n",
        "        \"\"\"Generate appropriate response based on similarity threshold\"\"\"\n",
        "        try:\n",
        "            if (state['max_similarity'] >= state['confidence_threshold'] and\n",
        "                state['similar_records'] and\n",
        "                not state.get('error')):\n",
        "\n",
        "                # Generate RAG response\n",
        "                filtered_records = [r for r in state['similar_records']\n",
        "                                  if r['similarity_score'] >= state['confidence_threshold']]\n",
        "\n",
        "                rag_response, trace_id = self.rag_system.generate_rag_response(\n",
        "                    state['processed_query'],\n",
        "                    filtered_records,\n",
        "                    state['medical_codes']\n",
        "                )\n",
        "\n",
        "                state['rag_response'] = rag_response\n",
        "                state['final_response'] = f\"🏥 **RAG Response** (Max similarity: {state['max_similarity']:.3f})\\n\\n{rag_response}\"\n",
        "                state['use_fallback'] = False\n",
        "                state['trace_id'] = trace_id\n",
        "                state['workflow_status'].append(\"✓ Generated RAG response\")\n",
        "\n",
        "            else:\n",
        "                # Generate fallback response\n",
        "                fallback_reason = (\n",
        "                    f\"Similarity {state['max_similarity']:.3f} below threshold {state['confidence_threshold']:.3f}\"\n",
        "                    if state['similar_records'] else \"No similar records found\"\n",
        "                )\n",
        "\n",
        "                if state.get('error'):\n",
        "                    fallback_reason = f\"Error in workflow: {state['error']}\"\n",
        "\n",
        "                fallback_response, trace_id = self.rag_system.generate_fallback_response(state['original_query'])\n",
        "\n",
        "                state['fallback_response'] = fallback_response\n",
        "                state['final_response'] = f\"🤖 **Fallback Response** ({fallback_reason})\\n\\n{fallback_response}\"\n",
        "                state['use_fallback'] = True\n",
        "                state['fallback_reason'] = fallback_reason\n",
        "                state['trace_id'] = trace_id\n",
        "                state['workflow_status'].append(\"✓ Generated fallback response\")\n",
        "\n",
        "        except Exception as e:\n",
        "            # Error fallback\n",
        "            error_response = f\"❌ Error generating response: {str(e)}\"\n",
        "            state['final_response'] = error_response\n",
        "            state['use_fallback'] = True\n",
        "            state['fallback_reason'] = f\"Generation error: {str(e)}\"\n",
        "            state['error'] = str(e)\n",
        "\n",
        "        return state\n",
        "\n",
        "class MedicalRAGWorkflow:\n",
        "    \"\"\"LangGraph workflow orchestrating the medical RAG process\"\"\"\n",
        "\n",
        "    def __init__(self, rag_system):\n",
        "        self.rag_system = rag_system\n",
        "\n",
        "        # Initialize agents\n",
        "        self.query_agent = QueryAnalysisAgent(rag_system.generative_model)\n",
        "        self.retrieval_agent = RetrievalAgent(rag_system)\n",
        "        self.coding_agent = MedicalCodingAgent(rag_system)\n",
        "        self.response_agent = ResponseGenerationAgent(rag_system)\n",
        "\n",
        "        # Build workflow\n",
        "        self.workflow = self._build_workflow()\n",
        "\n",
        "    def _build_workflow(self):\n",
        "        \"\"\"Build the LangGraph workflow\"\"\"\n",
        "        if not LANGGRAPH_ENABLED:\n",
        "            return None\n",
        "\n",
        "        # Create workflow\n",
        "        workflow = StateGraph(WorkflowState)\n",
        "\n",
        "        # Add nodes\n",
        "        workflow.add_node(\"query_analysis\", self._query_analysis_node)\n",
        "        workflow.add_node(\"retrieval\", self._retrieval_node)\n",
        "        workflow.add_node(\"medical_coding\", self._medical_coding_node)\n",
        "        workflow.add_node(\"quality_check\", self._quality_check_node)\n",
        "        workflow.add_node(\"response_generation\", self._response_generation_node)\n",
        "        workflow.add_node(\"final_processing\", self._final_processing_node)\n",
        "\n",
        "        # Define workflow edges\n",
        "        workflow.set_entry_point(\"query_analysis\")\n",
        "        workflow.add_edge(\"query_analysis\", \"retrieval\")\n",
        "        workflow.add_edge(\"retrieval\", \"medical_coding\")\n",
        "        workflow.add_edge(\"medical_coding\", \"quality_check\")\n",
        "        workflow.add_edge(\"quality_check\", \"response_generation\")\n",
        "        workflow.add_edge(\"response_generation\", \"final_processing\")\n",
        "        workflow.add_edge(\"final_processing\", END)\n",
        "\n",
        "        # Compile workflow\n",
        "        memory = MemorySaver()\n",
        "        return workflow.compile(checkpointer=memory)\n",
        "\n",
        "    def _query_analysis_node(self, state: WorkflowState) -> WorkflowState:\n",
        "        \"\"\"Node for query analysis\"\"\"\n",
        "        return self.query_agent.analyze_query(state)\n",
        "\n",
        "    def _retrieval_node(self, state: WorkflowState) -> WorkflowState:\n",
        "        \"\"\"Node for document retrieval\"\"\"\n",
        "        return self.retrieval_agent.retrieve_records(state)\n",
        "\n",
        "    def _medical_coding_node(self, state: WorkflowState) -> WorkflowState:\n",
        "        \"\"\"Node for medical code extraction\"\"\"\n",
        "        return self.coding_agent.extract_codes(state)\n",
        "\n",
        "    def _quality_check_node(self, state: WorkflowState) -> WorkflowState:\n",
        "        \"\"\"Node for quality validation of retrieved results\"\"\"\n",
        "        try:\n",
        "            # Additional quality checks based on query intent\n",
        "            if state['query_intent'] == 'diagnostic' and state['max_similarity'] > 0:\n",
        "                # Boost threshold for diagnostic queries\n",
        "                if state['max_similarity'] < 0.8:\n",
        "                    state['workflow_status'].append(\"⚠ Diagnostic query - similarity below recommended threshold\")\n",
        "\n",
        "            elif state['query_intent'] == 'code-lookup':\n",
        "                # For code lookup, check if we found relevant codes\n",
        "                total_codes = sum(len(codes) for codes in state['medical_codes'].values())\n",
        "                if total_codes == 0 and state['max_similarity'] < 0.6:\n",
        "                    state['workflow_status'].append(\"⚠ Code lookup - no relevant codes found\")\n",
        "\n",
        "            # Quality score based on similarity and intent match\n",
        "            quality_score = state['max_similarity']\n",
        "            if state['query_intent'] in ['diagnostic', 'procedural'] and quality_score > 0.7:\n",
        "                quality_score += 0.1  # Boost for clinical queries with good similarity\n",
        "\n",
        "            state['workflow_status'].append(f\"✓ Quality check completed - Score: {quality_score:.3f}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            state['error'] = f\"Quality check error: {str(e)}\"\n",
        "\n",
        "        return state\n",
        "\n",
        "    def _response_generation_node(self, state: WorkflowState) -> WorkflowState:\n",
        "        \"\"\"Node for response generation\"\"\"\n",
        "        return self.response_agent.generate_response(state)\n",
        "\n",
        "    def _final_processing_node(self, state: WorkflowState) -> WorkflowState:\n",
        "        \"\"\"Node for final processing and cleanup\"\"\"\n",
        "        try:\n",
        "            # Add workflow completion status\n",
        "            state['workflow_status'].append(\"✓ Workflow completed successfully\")\n",
        "\n",
        "            # Ensure all required fields are present\n",
        "            if not state.get('final_response'):\n",
        "                state['final_response'] = \"❌ No response generated\"\n",
        "                state['use_fallback'] = True\n",
        "                state['fallback_reason'] = \"Final processing error\"\n",
        "\n",
        "        except Exception as e:\n",
        "            state['error'] = f\"Final processing error: {str(e)}\"\n",
        "            state['final_response'] = f\"❌ Workflow error: {str(e)}\"\n",
        "\n",
        "        return state\n",
        "\n",
        "    def execute(self, user_query: str, confidence_threshold: float, top_k: int, file_context: str = \"\") -> Dict[str, Any]:\n",
        "        \"\"\"Execute the workflow\"\"\"\n",
        "        if not self.workflow:\n",
        "            # Fallback to original method if LangGraph not available\n",
        "            return self._execute_fallback(user_query, confidence_threshold, top_k, file_context)\n",
        "\n",
        "        try:\n",
        "            # Initialize state\n",
        "            initial_state = WorkflowState(\n",
        "                user_query=user_query,\n",
        "                original_query=user_query,\n",
        "                processed_query=user_query,\n",
        "                query_intent='general',\n",
        "                confidence_threshold=confidence_threshold,\n",
        "                top_k=top_k,\n",
        "                file_context=file_context,\n",
        "                similar_records=[],\n",
        "                medical_codes={'diagnoses': {}, 'procedures': {}, 'cpt_codes': {}},\n",
        "                rag_response='',\n",
        "                fallback_response='',\n",
        "                final_response='',\n",
        "                use_fallback=False,\n",
        "                fallback_reason='',\n",
        "                max_similarity=0.0,\n",
        "                workflow_status=['🚀 Workflow started'],\n",
        "                trace_id=None,\n",
        "                error=None\n",
        "            )\n",
        "\n",
        "            # Execute workflow\n",
        "            config = {\"configurable\": {\"thread_id\": str(uuid.uuid4())}}\n",
        "            result = self.workflow.invoke(initial_state, config)\n",
        "\n",
        "            # Return formatted result\n",
        "            return {\n",
        "                'success': True,\n",
        "                'use_fallback': result['use_fallback'],\n",
        "                'fallback_reason': result.get('fallback_reason', ''),\n",
        "                'response': result['final_response'],\n",
        "                'medical_codes': result.get('medical_codes', {}),\n",
        "                'max_similarity': result.get('max_similarity', 0.0),\n",
        "                'total_records_found': len(result.get('similar_records', [])),\n",
        "                'records_above_threshold': len([r for r in result.get('similar_records', [])\n",
        "                                              if r.get('similarity_score', 0) >= confidence_threshold]),\n",
        "                'trace_id': result.get('trace_id'),\n",
        "                'workflow_status': result.get('workflow_status', []),\n",
        "                'query_intent': result.get('query_intent', 'general')\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            return {\n",
        "                'success': False,\n",
        "                'error': f\"Workflow execution error: {str(e)}\",\n",
        "                'use_fallback': True,\n",
        "                'trace_id': None\n",
        "            }\n",
        "\n",
        "    def _execute_fallback(self, user_query: str, confidence_threshold: float, top_k: int, file_context: str = \"\") -> Dict[str, Any]:\n",
        "        \"\"\"Fallback execution when LangGraph is not available\"\"\"\n",
        "        return self.rag_system.query_with_threshold(user_query, confidence_threshold, top_k)\n",
        "\n",
        "class IntegratedMedicalRAG:\n",
        "    \"\"\"Enhanced Medical RAG system with multi-modal input support and Langfuse tracking\"\"\"\n",
        "\n",
        "    def __init__(self, project_id: str, location: str, bucket_name: str, langfuse: Optional[Langfuse] = None):\n",
        "        self.project_id = project_id\n",
        "        self.location = location\n",
        "        self.bucket_name = bucket_name\n",
        "        self.langfuse = langfuse\n",
        "\n",
        "        try:\n",
        "            # Initialize Vertex AI\n",
        "            vertexai.init(project=project_id, location=location)\n",
        "            self.embedding_model = TextEmbeddingModel.from_pretrained('text-embedding-004')\n",
        "            self.generative_model = GenerativeModel('gemini-2.5-flash')\n",
        "            self.storage_client = storage.Client(project=project_id)\n",
        "            self.bucket = self.storage_client.bucket(bucket_name)\n",
        "\n",
        "            self.medical_records = []\n",
        "            self.is_initialized = True\n",
        "\n",
        "            # Initialize LangGraph workflow\n",
        "            self.workflow_engine = MedicalRAGWorkflow(self) if LANGGRAPH_ENABLED else None\n",
        "\n",
        "        except Exception as e:\n",
        "            st.error(f\"Failed to initialize Vertex AI: {e}\")\n",
        "            self.is_initialized = False\n",
        "\n",
        "    def load_embeddings_from_gcs(self):\n",
        "        \"\"\"Load embeddings from GCS\"\"\"\n",
        "        try:\n",
        "            gcs_path = \"embeddings/medical_embeddings.jsonl\"\n",
        "            blob = self.bucket.blob(gcs_path)\n",
        "            local_path = \"/tmp/medical_embeddings.jsonl\"\n",
        "            blob.download_to_filename(local_path)\n",
        "\n",
        "            self.medical_records = []\n",
        "            with open(local_path, 'r') as f:\n",
        "                for line in f:\n",
        "                    record = json.loads(line.strip())\n",
        "                    self.medical_records.append(record)\n",
        "\n",
        "            return True, f\"Loaded {len(self.medical_records)} medical records\"\n",
        "        except Exception as e:\n",
        "            return False, f\"Error loading embeddings: {e}\"\n",
        "\n",
        "    def search_similar_records(self, query: str, top_k: int = 10):\n",
        "        \"\"\"Search for similar medical records\"\"\"\n",
        "        if not self.medical_records:\n",
        "            return []\n",
        "\n",
        "        try:\n",
        "            # Generate query embedding\n",
        "            query_embeddings = self.embedding_model.get_embeddings([query])\n",
        "            query_vector = np.array(query_embeddings[0].values)\n",
        "\n",
        "            # Get all record embeddings\n",
        "            record_embeddings = np.array([record['embedding'] for record in self.medical_records])\n",
        "\n",
        "            # Calculate similarities\n",
        "            similarities = cosine_similarity([query_vector], record_embeddings)[0]\n",
        "\n",
        "            # Get top-k similar records\n",
        "            top_indices = np.argsort(similarities)[-top_k:][::-1]\n",
        "\n",
        "            similar_records = []\n",
        "            for idx in top_indices:\n",
        "                record = self.medical_records[idx].copy()\n",
        "                record['similarity_score'] = float(similarities[idx])\n",
        "                similar_records.append(record)\n",
        "\n",
        "            return similar_records\n",
        "        except Exception as e:\n",
        "            st.error(f\"Error in similarity search: {e}\")\n",
        "            return []\n",
        "\n",
        "    def extract_medical_codes_from_results(self, similar_records: List[Dict[str, Any]]):\n",
        "        \"\"\"Extract medical codes from search results\"\"\"\n",
        "        medical_codes = {\n",
        "            'diagnoses': {},\n",
        "            'procedures': {},\n",
        "            'cpt_codes': {}\n",
        "        }\n",
        "\n",
        "        for record in similar_records:\n",
        "            # Process diagnoses\n",
        "            for diag in record.get('diagnoses', []):\n",
        "                code = diag.get('ICD9_CODE')\n",
        "                if code:\n",
        "                    if code not in medical_codes['diagnoses']:\n",
        "                        medical_codes['diagnoses'][code] = {\n",
        "                            'short_title': diag.get('SHORT_TITLE', ''),\n",
        "                            'long_title': diag.get('LONG_TITLE', ''),\n",
        "                            'frequency': 0,\n",
        "                            'max_similarity': 0\n",
        "                        }\n",
        "                    medical_codes['diagnoses'][code]['frequency'] += 1\n",
        "                    medical_codes['diagnoses'][code]['max_similarity'] = max(\n",
        "                        medical_codes['diagnoses'][code]['max_similarity'],\n",
        "                        record['similarity_score']\n",
        "                    )\n",
        "\n",
        "            # Process procedures\n",
        "            for proc in record.get('procedures', []):\n",
        "                code = str(proc.get('ICD9_CODE'))\n",
        "                if code and code != \"None\":\n",
        "                    if code not in medical_codes['procedures']:\n",
        "                        medical_codes['procedures'][code] = {\n",
        "                            'short_title': proc.get('SHORT_TITLE', ''),\n",
        "                            'long_title': proc.get('LONG_TITLE', ''),\n",
        "                            'frequency': 0,\n",
        "                            'max_similarity': 0\n",
        "                        }\n",
        "                    medical_codes['procedures'][code]['frequency'] += 1\n",
        "                    medical_codes['procedures'][code]['max_similarity'] = max(\n",
        "                        medical_codes['procedures'][code]['max_similarity'],\n",
        "                        record['similarity_score']\n",
        "                    )\n",
        "\n",
        "            # Process CPT codes\n",
        "            for cpt in record.get('cpt_codes', []):\n",
        "                code = cpt.get('CPT_CD')\n",
        "                if code:\n",
        "                    if code not in medical_codes['cpt_codes']:\n",
        "                        medical_codes['cpt_codes'][code] = {\n",
        "                            'description': cpt.get('DESCRIPTION', ''),\n",
        "                            'section': cpt.get('SECTIONHEADER', ''),\n",
        "                            'subsection': cpt.get('SUBSECTIONHEADER', ''),\n",
        "                            'frequency': 0,\n",
        "                            'max_similarity': 0\n",
        "                        }\n",
        "                    medical_codes['cpt_codes'][code]['frequency'] += 1\n",
        "                    medical_codes['cpt_codes'][code]['max_similarity'] = max(\n",
        "                        medical_codes['cpt_codes'][code]['max_similarity'],\n",
        "                        record['similarity_score']\n",
        "                    )\n",
        "\n",
        "        return medical_codes\n",
        "\n",
        "    def generate_rag_response(self, query: str, similar_records: List[Dict[str, Any]], medical_codes: Dict[str, Dict]):\n",
        "        \"\"\"Generate response using RAG with Langfuse tracking\"\"\"\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Prepare context from top records\n",
        "        context = \"\\n\\n\".join([f\"Record {i+1}: {record['content'][:300]}...\"\n",
        "                                for i, record in enumerate(similar_records[:3])])\n",
        "\n",
        "        # Format medical codes\n",
        "        diagnoses_text = \"\"\n",
        "        if medical_codes['diagnoses']:\n",
        "            diagnoses_text = \"\\nDIAGNOSIS CODES FOUND:\\n\"\n",
        "            for code, info in sorted(medical_codes['diagnoses'].items(),\n",
        "                                   key=lambda x: x[1]['max_similarity'], reverse=True)[:5]:\n",
        "                diagnoses_text += f\"- {code}: {info['long_title']} (freq: {info['frequency']}, sim: {info['max_similarity']:.3f})\\n\"\n",
        "\n",
        "        procedures_text = \"\"\n",
        "        if medical_codes['procedures']:\n",
        "            procedures_text = \"\\nPROCEDURE CODES FOUND:\\n\"\n",
        "            for code, info in sorted(medical_codes['procedures'].items(),\n",
        "                                   key=lambda x: x[1]['max_similarity'], reverse=True)[:5]:\n",
        "                procedures_text += f\"- {code}: {info['long_title']} (freq: {info['frequency']}, sim: {info['max_similarity']:.3f})\\n\"\n",
        "\n",
        "        cpt_text = \"\"\n",
        "        if medical_codes['cpt_codes']:\n",
        "            cpt_text = \"\\nCPT CODES FOUND:\\n\"\n",
        "            for code, info in sorted(medical_codes['cpt_codes'].items(),\n",
        "                                   key=lambda x: x[1]['max_similarity'], reverse=True)[:5]:\n",
        "                cpt_text += f\"- {code}: {info['description']} (freq: {info['frequency']}, sim: {info['max_similarity']:.3f})\\n\"\n",
        "\n",
        "        prompt = f\"\"\"You are a medical coding assistant. Based on the following medical records context and the user's query, provide relevant medical codes and explanations.\n",
        "\n",
        "CONTEXT FROM MEDICAL RECORDS:\n",
        "{context}\n",
        "\n",
        "{diagnoses_text}\n",
        "{procedures_text}\n",
        "{cpt_text}\n",
        "\n",
        "USER QUERY: {query}\n",
        "\n",
        "Please provide a comprehensive response that:\n",
        "1. Explains what medical codes are most relevant to the query\n",
        "2. Provides context on why these codes match the query\n",
        "3. Lists the specific ICD-9 diagnosis codes, procedure codes, and CPT codes\n",
        "4. Explains what each code represents in simple terms\n",
        "5. Notes the similarity scores to show relevance\n",
        "\n",
        "Keep your response concise and well-formatted.\"\"\"\n",
        "\n",
        "        try:\n",
        "            if self.langfuse:\n",
        "                with self.langfuse.start_as_current_span(name=\"rag-response\", input={\"user_query\": query}) as span:\n",
        "                    with self.langfuse.start_as_current_generation(\n",
        "                        name=\"gemini-rag-generation\",\n",
        "                        model=\"gemini-2.5-flash\",\n",
        "                        input={\"prompt\": prompt}\n",
        "                    ) as generation:\n",
        "                        response = self.generative_model.generate_content(prompt)\n",
        "                        response_text = response.text\n",
        "                        processing_time = time.time() - start_time\n",
        "\n",
        "                        generation.update(output=response_text)\n",
        "                        span.update_trace(output={\n",
        "                            \"response\": response_text,\n",
        "                            \"processing_time\": processing_time,\n",
        "                            \"response_type\": \"RAG\",\n",
        "                            \"records_used\": len(similar_records),\n",
        "                            \"medical_codes_found\": sum(len(codes) for codes in medical_codes.values())\n",
        "                        })\n",
        "\n",
        "                        return response_text, span.trace_id\n",
        "            else:\n",
        "                response = self.generative_model.generate_content(prompt)\n",
        "                return response.text, None\n",
        "        except Exception as e:\n",
        "            error_msg = f\"Error generating RAG response: {e}\"\n",
        "            if self.langfuse:\n",
        "                with self.langfuse.start_as_current_span(name=\"rag-response-error\", input={\"user_query\": query}) as span:\n",
        "                    span.update_trace(output={\"error\": error_msg})\n",
        "                    return error_msg, span.trace_id\n",
        "            return error_msg, None\n",
        "\n",
        "    def generate_fallback_response(self, query: str):\n",
        "        \"\"\"Generate fallback response using Gemini when confidence is low with Langfuse tracking\"\"\"\n",
        "        start_time = time.time()\n",
        "\n",
        "        prompt = f\"\"\"You are a medical coding assistant. The user has asked about: {query}\n",
        "\n",
        "Since no highly relevant medical records were found in the database, please provide a general medical coding response that includes:\n",
        "\n",
        "1. Common ICD-9 diagnosis codes that might be related to this query\n",
        "2. Typical CPT codes for procedures related to this condition\n",
        "3. Brief explanations of what these codes represent\n",
        "4. General medical context for the query\n",
        "\n",
        "Please format your response clearly with:\n",
        "- DIAGNOSIS CODES (ICD-9): List relevant codes with descriptions\n",
        "- PROCEDURE CODES: List relevant CPT codes with descriptions\n",
        "- MEDICAL CONTEXT: Brief explanation of the condition/symptoms\n",
        "\n",
        "Remember to advise consultation with healthcare professionals for serious medical concerns.\"\"\"\n",
        "\n",
        "        try:\n",
        "            if self.langfuse:\n",
        "                with self.langfuse.start_as_current_span(name=\"fallback-response\", input={\"user_query\": query}) as span:\n",
        "                    with self.langfuse.start_as_current_generation(\n",
        "                        name=\"gemini-fallback-generation\",\n",
        "                        model=\"gemini-2.5-flash\",\n",
        "                        input={\"prompt\": prompt}\n",
        "                    ) as generation:\n",
        "                        response = self.generative_model.generate_content(prompt)\n",
        "                        response_text = response.text\n",
        "                        processing_time = time.time() - start_time\n",
        "\n",
        "                        generation.update(output=response_text)\n",
        "                        span.update_trace(output={\n",
        "                            \"response\": response_text,\n",
        "                            \"processing_time\": processing_time,\n",
        "                            \"response_type\": \"Fallback\"\n",
        "                        })\n",
        "\n",
        "                        return response_text, span.trace_id\n",
        "            else:\n",
        "                response = self.generative_model.generate_content(prompt)\n",
        "                return response.text, None\n",
        "        except Exception as e:\n",
        "            error_msg = f\"Error generating fallback response: {e}\"\n",
        "            if self.langfuse:\n",
        "                with self.langfuse.start_as_current_span(name=\"fallback-response-error\", input={\"user_query\": query}) as span:\n",
        "                    span.update_trace(output={\"error\": error_msg})\n",
        "                    return error_msg, span.trace_id\n",
        "            return error_msg, None\n",
        "\n",
        "    def query_with_threshold(self, user_query: str, confidence_threshold: float, top_k: int = 10):\n",
        "        \"\"\"Main query function with confidence threshold and Langfuse tracking\"\"\"\n",
        "        if not self.is_initialized:\n",
        "            return {\n",
        "                'success': False,\n",
        "                'error': \"RAG system not initialized\",\n",
        "                'use_fallback': True,\n",
        "                'trace_id': None\n",
        "            }\n",
        "\n",
        "        # Use LangGraph workflow if available\n",
        "        if self.workflow_engine:\n",
        "            file_context = st.session_state.get('uploaded_files_text', '')\n",
        "            return self.workflow_engine.execute(user_query, confidence_threshold, top_k, file_context)\n",
        "\n",
        "        # Fallback to original implementation\n",
        "        return self._original_query_implementation(user_query, confidence_threshold, top_k)\n",
        "\n",
        "    def _original_query_implementation(self, user_query: str, confidence_threshold: float, top_k: int = 10):\n",
        "        \"\"\"Original query implementation as fallback\"\"\"\n",
        "        # Search for similar records\n",
        "        similar_records = self.search_similar_records(user_query, top_k)\n",
        "        if not similar_records:\n",
        "            response, trace_id = self.generate_fallback_response(user_query)\n",
        "            return {\n",
        "                'success': True,\n",
        "                'use_fallback': True,\n",
        "                'fallback_reason': \"No similar records found\",\n",
        "                'response': response,\n",
        "                'trace_id': trace_id\n",
        "            }\n",
        "\n",
        "        # Check highest similarity score against threshold\n",
        "        max_similarity = max([record['similarity_score'] for record in similar_records])\n",
        "        if max_similarity < confidence_threshold:\n",
        "            response, trace_id = self.generate_fallback_response(user_query)\n",
        "            return {\n",
        "                'success': True,\n",
        "                'use_fallback': True,\n",
        "                'fallback_reason': f\"Highest similarity {max_similarity:.3f} below threshold {confidence_threshold:.3f}\",\n",
        "                'response': response,\n",
        "                'max_similarity': max_similarity,\n",
        "                'trace_id': trace_id\n",
        "            }\n",
        "\n",
        "        # Use RAG response\n",
        "        filtered_records = [r for r in similar_records if r['similarity_score'] >= confidence_threshold]\n",
        "        medical_codes = self.extract_medical_codes_from_results(filtered_records)\n",
        "        response, trace_id = self.generate_rag_response(user_query, filtered_records, medical_codes)\n",
        "\n",
        "        return {\n",
        "            'success': True,\n",
        "            'use_fallback': False,\n",
        "            'response': response,\n",
        "            'medical_codes': medical_codes,\n",
        "            'filtered_records': filtered_records,\n",
        "            'max_similarity': max_similarity,\n",
        "            'total_records_found': len(similar_records),\n",
        "            'records_above_threshold': len(filtered_records),\n",
        "            'trace_id': trace_id\n",
        "        }\n",
        "\n",
        "# File processing functions\n",
        "def extract_text_from_pdf(file_path: str) -> str:\n",
        "    \"\"\"Extract text from PDF file\"\"\"\n",
        "    try:\n",
        "        with open(file_path, 'rb') as file:\n",
        "            pdf_reader = PyPDF2.PdfReader(file)\n",
        "            text = \"\"\n",
        "            for page in pdf_reader.pages:\n",
        "                text += page.extract_text()\n",
        "        return text.strip()\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error reading PDF: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "def extract_text_from_docx(file_path: str) -> str:\n",
        "    \"\"\"Extract text from DOCX file\"\"\"\n",
        "    try:\n",
        "        doc = docx.Document(file_path)\n",
        "        text = \"\"\n",
        "        for paragraph in doc.paragraphs:\n",
        "            text += paragraph.text\n",
        "        return text.strip()\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error reading DOCX: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "def read_text_file_with_fallback(file_path: str) -> str:\n",
        "    \"\"\"Read text file with multiple encoding fallbacks\"\"\"\n",
        "    encodings_to_try = ['utf-8', 'utf-8-sig', 'latin-1', 'cp1252', 'iso-8859-1']\n",
        "\n",
        "    for encoding in encodings_to_try:\n",
        "        try:\n",
        "            with open(file_path, 'r', encoding=encoding) as f:\n",
        "                content = f.read().strip()\n",
        "                if content:  # Return if content is found\n",
        "                    return content\n",
        "        except (UnicodeDecodeError, Exception):\n",
        "            continue\n",
        "\n",
        "    # If all encodings fail, try binary read\n",
        "    try:\n",
        "        with open(file_path, 'rb') as f:\n",
        "            content = f.read()\n",
        "            decoded_content = content.decode('utf-8', errors='ignore').strip()\n",
        "            if decoded_content:\n",
        "                return decoded_content\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    return \"\"  # Return empty string instead of error message\n",
        "\n",
        "def process_single_file(file) -> dict:\n",
        "    \"\"\"Process a single uploaded file and return file info with extracted text\"\"\"\n",
        "    try:\n",
        "        with tempfile.NamedTemporaryFile(delete=False, suffix=os.path.splitext(file.name)[1]) as tmp_file:\n",
        "            tmp_file.write(file.getvalue())\n",
        "            tmp_file_path = tmp_file.name\n",
        "\n",
        "        file_extension = os.path.splitext(file.name)[1].lower()\n",
        "        text = \"\"\n",
        "        file_type = \"Unknown\"\n",
        "\n",
        "        if file.type == \"application/pdf\" or file_extension == \".pdf\":\n",
        "            text = extract_text_from_pdf(tmp_file_path)\n",
        "            file_type = \"PDF\"\n",
        "        elif file.type == \"application/vnd.openxmlformats-officedocument.wordprocessingml.document\" or file_extension == \".docx\":\n",
        "            text = extract_text_from_docx(tmp_file_path)\n",
        "            file_type = \"Word Document\"\n",
        "        elif (file.type and file.type.startswith(\"text\")) or file_extension in [\".txt\", \".text\", \".log\", \".md\", \".csv\"]:\n",
        "            text = read_text_file_with_fallback(tmp_file_path)\n",
        "            file_type = \"Text File\"\n",
        "        else:\n",
        "            # Fallback: try to read as text file\n",
        "            text = read_text_file_with_fallback(tmp_file_path)\n",
        "            if text and len(text.strip()) > 0:\n",
        "                file_type = \"Text File (Auto-detected)\"\n",
        "            else:\n",
        "                text = f\"Unsupported file type: {file.name}\"\n",
        "                file_type = \"Unsupported\"\n",
        "\n",
        "        os.unlink(tmp_file_path)\n",
        "\n",
        "        status = \"success\" if text and text.strip() and not text.startswith(\"Unsupported\") else \"failed\"\n",
        "\n",
        "        return {\n",
        "            \"name\": file.name,\n",
        "            \"type\": file_type,\n",
        "            \"size\": len(file.getvalue()),\n",
        "            \"text\": text,\n",
        "            \"status\": status,\n",
        "            \"mimetype\": file.type if hasattr(file, \"type\") else \"unknown\",\n",
        "            \"extension\": file_extension,\n",
        "            \"text_length\": len(text) if text else 0\n",
        "        }\n",
        "    except Exception as e:\n",
        "        return {\n",
        "            \"name\": file.name,\n",
        "            \"type\": \"Unknown\",\n",
        "            \"size\": 0,\n",
        "            \"text\": \"\",\n",
        "            \"status\": \"error\",\n",
        "            \"error\": str(e),\n",
        "            \"mimetype\": file.type if hasattr(file, \"type\") else \"unknown\",\n",
        "            \"extension\": os.path.splitext(file.name)[1].lower() if hasattr(file, \"name\") else \"unknown\",\n",
        "            \"text_length\": 0\n",
        "        }\n",
        "\n",
        "def text_to_speech_base64(text: str, language: str = \"en\") -> str:\n",
        "    \"\"\"Convert text to speech and return base64 encoded audio\"\"\"\n",
        "    try:\n",
        "        if len(text) > 500:\n",
        "            text_for_tts = text[:500] + \"...\"\n",
        "        else:\n",
        "            text_for_tts = text\n",
        "\n",
        "        tts = gTTS(text=text_for_tts, lang=language, slow=False)\n",
        "\n",
        "        with tempfile.NamedTemporaryFile(suffix='.mp3', delete=False) as tmp_file:\n",
        "            tts.save(tmp_file.name)\n",
        "\n",
        "            with open(tmp_file.name, 'rb') as audio_file:\n",
        "                audio_bytes = audio_file.read()\n",
        "                audio_base64 = base64.b64encode(audio_bytes).decode()\n",
        "\n",
        "            os.unlink(tmp_file.name)\n",
        "            return audio_base64\n",
        "\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error generating speech: {e}\")\n",
        "        return None\n",
        "\n",
        "def improved_voice_transcription(audio):\n",
        "    \"\"\"Improved voice transcription with better error handling\"\"\"\n",
        "    try:\n",
        "        if not AUDIO_ENABLED:\n",
        "            return \"Audio features not available\"\n",
        "\n",
        "        if st.session_state.whisper_model is None:\n",
        "            st.session_state.whisper_model = load_whisper_model()\n",
        "            if st.session_state.whisper_model is None:\n",
        "                return \"Whisper model not available\"\n",
        "\n",
        "        w = st.session_state.whisper_model\n",
        "\n",
        "        # Create temporary file with proper cleanup\n",
        "        with NamedTemporaryFile(suffix=\".wav\", delete=False) as temp:\n",
        "            # Export as WAV for better compatibility\n",
        "            audio.export(temp.name, format=\"wav\")\n",
        "\n",
        "            # Transcribe\n",
        "            result = w.transcribe(temp.name)\n",
        "            text = w.extract_text(result)\n",
        "\n",
        "            # Clean up temp file\n",
        "            os.unlink(temp.name)\n",
        "\n",
        "            # Return first result or empty if none\n",
        "            if text and len(text) > 0:\n",
        "                transcribed = text[0].strip()\n",
        "                # Filter out common transcription artifacts\n",
        "                if len(transcribed) > 3 and not transcribed.lower().startswith(\"you\"):\n",
        "                    return transcribed\n",
        "                else:\n",
        "                    return \"Please speak more clearly and try again\"\n",
        "            else:\n",
        "                return \"No speech detected. Please try again\"\n",
        "\n",
        "    except Exception as e:\n",
        "        st.error(f\"Transcription error: {e}\")\n",
        "        return f\"Transcription failed: {str(e)}\"\n",
        "\n",
        "def highlight_medical_codes(text: str) -> str:\n",
        "    \"\"\"Highlight medical codes in the response text\"\"\"\n",
        "    # Pattern to match medical codes (ICD-9, CPT, etc.)\n",
        "    patterns = [\n",
        "        r'\\b\\d{1,3}\\.\\d{1,2}\\b',  # ICD-9 codes like 250.00\n",
        "        r'\\b\\d{5}\\b',  # CPT codes\n",
        "        r'\\b[A-Z]\\d{2}\\b',  # Some ICD codes\n",
        "        r'\\b\\d{3}-\\d{3}-\\d{4}\\b'  # Other medical codes\n",
        "    ]\n",
        "\n",
        "    highlighted_text = text\n",
        "    for pattern in patterns:\n",
        "        highlighted_text = re.sub(pattern, lambda m: f'<span class=\"code-highlight\">{m.group()}</span>', highlighted_text)\n",
        "\n",
        "    return highlighted_text\n",
        "\n",
        "def display_medical_codes_box(medical_codes: Dict[str, Dict]):\n",
        "    \"\"\"Display medical codes in a highlighted box\"\"\"\n",
        "    if not medical_codes or not any(medical_codes.values()):\n",
        "        return\n",
        "\n",
        "    codes_html = '<div class=\"medical-code-box\"><h4>📋 Medical Codes Found</h4>'\n",
        "\n",
        "    # Diagnosis codes\n",
        "    if medical_codes.get('diagnoses'):\n",
        "        codes_html += '<h5>🏥 Diagnosis Codes (ICD-9)</h5>'\n",
        "        for code, info in sorted(medical_codes['diagnoses'].items(),\n",
        "                               key=lambda x: x[1]['max_similarity'], reverse=True)[:5]:\n",
        "            codes_html += f'<p><span class=\"code-highlight\">{code}</span>: {info[\"long_title\"]}<br><small>Frequency: {info[\"frequency\"]}, Similarity: {info[\"max_similarity\"]:.3f}</small></p>'\n",
        "\n",
        "    # Procedure codes\n",
        "    if medical_codes.get('procedures'):\n",
        "        codes_html += '<h5>⚕️ Procedure Codes (ICD-9)</h5>'\n",
        "        for code, info in sorted(medical_codes['procedures'].items(),\n",
        "                               key=lambda x: x[1]['max_similarity'], reverse=True)[:5]:\n",
        "            codes_html += f'<p><span class=\"code-highlight\">{code}</span>: {info[\"long_title\"]}<br><small>Frequency: {info[\"frequency\"]}, Similarity: {info[\"max_similarity\"]:.3f}</small></p>'\n",
        "\n",
        "    # CPT codes\n",
        "    if medical_codes.get('cpt_codes'):\n",
        "        codes_html += '<h5>💊 CPT Codes</h5>'\n",
        "        for code, info in sorted(medical_codes['cpt_codes'].items(),\n",
        "                               key=lambda x: x[1]['max_similarity'], reverse=True)[:5]:\n",
        "            codes_html += f'<p><span class=\"code-highlight\">{code}</span>: {info[\"description\"]}<br><small>Frequency: {info[\"frequency\"]}, Similarity: {info[\"max_similarity\"]:.3f}</small></p>'\n",
        "\n",
        "    codes_html += '</div>'\n",
        "    st.markdown(codes_html, unsafe_allow_html=True)\n",
        "\n",
        "def display_workflow_status(workflow_status: List[str]):\n",
        "    \"\"\"Display workflow execution status\"\"\"\n",
        "    if workflow_status and LANGGRAPH_ENABLED:\n",
        "        status_html = '<div class=\"workflow-status\"><h5>🔄 Workflow Status</h5><ul>'\n",
        "        for status in workflow_status:\n",
        "            status_html += f'<li>{status}</li>'\n",
        "        status_html += '</ul></div>'\n",
        "        st.markdown(status_html, unsafe_allow_html=True)\n",
        "\n",
        "\n",
        "def execute_rag_query(prompt, confidence_threshold, top_k):\n",
        "    \"\"\"Execute RAG query and handle the response\"\"\"\n",
        "    if not st.session_state.rag_system or not st.session_state.embeddings_loaded:\n",
        "        error_msg = \"⚠️ RAG system not ready. Please wait for initialization or check the sidebar.\"\n",
        "        st.error(error_msg)\n",
        "        st.session_state.messages.append({\"role\": \"assistant\", \"content\": error_msg})\n",
        "        return\n",
        "\n",
        "    with st.spinner(\"🔥 Processing your query...\"):\n",
        "        # Get RAG response (now with LangGraph workflow)\n",
        "        result = st.session_state.rag_system.query_with_threshold(\n",
        "            prompt,\n",
        "            confidence_threshold,\n",
        "            top_k\n",
        "        )\n",
        "\n",
        "        if not result['success']:\n",
        "            response = f\"❌ Error: {result['error']}\"\n",
        "            st.error(response)\n",
        "            st.session_state.messages.append({\"role\": \"assistant\", \"content\": response})\n",
        "        elif result['use_fallback']:\n",
        "            response = result['response']   # already formatted in workflow\n",
        "            st.markdown(response)\n",
        "\n",
        "\n",
        "            # Display workflow status if available\n",
        "            if result.get('workflow_status'):\n",
        "                display_workflow_status(result['workflow_status'])\n",
        "\n",
        "            st.session_state.messages.append({\n",
        "                \"role\": \"assistant\",\n",
        "                \"content\": response,\n",
        "                \"trace_id\": result.get('trace_id'),\n",
        "                \"workflow_status\": result.get('workflow_status', [])\n",
        "            })\n",
        "\n",
        "            if result.get('trace_id') and st.session_state.langfuse:\n",
        "                trace_url = f\"https://cloud.langfuse.com/project/cmfiuk7380001ad074uio1la1/traces/{result['trace_id']}\"\n",
        "                st.markdown(f\"[→ View Langfuse Trace]({trace_url})\")\n",
        "        else:\n",
        "            response = result['response']   # already formatted in workflow\n",
        "            highlighted_response = highlight_medical_codes(response)\n",
        "            st.markdown(highlighted_response, unsafe_allow_html=True)\n",
        "\n",
        "            if result.get('medical_codes'):\n",
        "                display_medical_codes_box(result['medical_codes'])\n",
        "\n",
        "            # Display workflow status if available\n",
        "            if result.get('workflow_status'):\n",
        "                display_workflow_status(result['workflow_status'])\n",
        "\n",
        "            st.session_state.messages.append({\n",
        "                \"role\": \"assistant\",\n",
        "                \"content\": response,\n",
        "                \"medical_codes\": result.get('medical_codes', {}),\n",
        "                \"similarity_score\": result['max_similarity'],\n",
        "                \"trace_id\": result.get('trace_id'),\n",
        "                \"workflow_status\": result.get('workflow_status', []),\n",
        "                \"query_intent\": result.get('query_intent', 'general')\n",
        "            })\n",
        "\n",
        "            st.info(f\"📊 Found {result['total_records_found']} similar records, {result['records_above_threshold']} above threshold\")\n",
        "\n",
        "            # Show query intent if available\n",
        "            if result.get('query_intent') and result['query_intent'] != 'general':\n",
        "                st.info(f\"🎯 Query Intent: {result['query_intent']}\")\n",
        "\n",
        "            if result.get('trace_id') and st.session_state.langfuse:\n",
        "                trace_url = f\"https://cloud.langfuse.com/project/cmfiuk7380001ad074uio1la1/traces/{result['trace_id']}\"\n",
        "                st.markdown(f\"[→ View Langfuse Trace]({trace_url})\")\n",
        "\n",
        "\n",
        "def check_documents_available():\n",
        "    \"\"\"Check if documents are available and return status\"\"\"\n",
        "    if not st.session_state.sidebar_files_processed:\n",
        "        return False, 0, \"\"\n",
        "\n",
        "    successful_files = [f for f in st.session_state.sidebar_files_processed.values() if f[\"status\"] == \"success\"]\n",
        "\n",
        "    if not successful_files:\n",
        "        return False, 0, \"\"\n",
        "\n",
        "    # Check if the text still exists\n",
        "    combined_text = \"\"\n",
        "    for file_info in successful_files:\n",
        "        if file_info[\"text\"] and not file_info[\"text\"].startswith(\"Unsupported\") and not file_info[\"text\"].startswith(\"Error\"):\n",
        "            combined_text += f\"\\n\\n--- Content from {file_info['name']} ---\\n{file_info['text']}\"\n",
        "\n",
        "    # Update session state with current combined text\n",
        "    st.session_state.uploaded_files_text = combined_text\n",
        "\n",
        "    return len(combined_text.strip()) > 0, len(successful_files), combined_text\n",
        "\n",
        "\n",
        "# Initialize session state with improved structure\n",
        "session_state_vars = [\n",
        "    ('messages', []),\n",
        "    ('rag_system', None),\n",
        "    ('embeddings_loaded', False),\n",
        "    ('whisper_model', None),\n",
        "    ('uploaded_files_text', ''),\n",
        "    ('current_uploaded_files', []),\n",
        "    ('sidebar_files_processed', {}),\n",
        "    ('voice_transcribed_text', ''),\n",
        "    ('is_recording', False),\n",
        "    ('audio_processed', False),\n",
        "    ('selected_sample_query', ''),\n",
        "    ('chat_input_key', 0),\n",
        "    ('pending_query', ''),\n",
        "    ('execute_pending', False),\n",
        "    ('langfuse', None)\n",
        "]\n",
        "\n",
        "for var, default_value in session_state_vars:\n",
        "    if var not in st.session_state:\n",
        "        st.session_state[var] = default_value\n",
        "\n",
        "\n",
        "def load_rag_system():\n",
        "    \"\"\"Initialize and load the RAG system\"\"\"\n",
        "    if st.session_state.rag_system is None:\n",
        "        with st.spinner(\"🔥 Initializing RAG system...\"):\n",
        "            st.session_state.rag_system = IntegratedMedicalRAG(\n",
        "                PROJECT_ID,\n",
        "                LOCATION,\n",
        "                BUCKET_NAME,\n",
        "                st.session_state.langfuse\n",
        "            )\n",
        "\n",
        "    if not st.session_state.embeddings_loaded and st.session_state.rag_system.is_initialized:\n",
        "        with st.spinner(\"📊 Loading medical embeddings from GCS...\"):\n",
        "            success, message = st.session_state.rag_system.load_embeddings_from_gcs()\n",
        "            st.session_state.embeddings_loaded = success\n",
        "            if success:\n",
        "                st.success(message)\n",
        "            else:\n",
        "                st.error(message)\n",
        "\n",
        "\n",
        "def handle_sample_query_selection(query):\n",
        "    \"\"\"Handle sample query selection from FAQ - directly execute RAG\"\"\"\n",
        "    st.session_state.pending_query = query\n",
        "    st.session_state.execute_pending = True\n",
        "    st.rerun()\n",
        "\n",
        "\n",
        "def handle_voice_input():\n",
        "    \"\"\"Handle voice input - directly execute RAG\"\"\"\n",
        "    if st.session_state.voice_transcribed_text:\n",
        "        query = st.session_state.voice_transcribed_text\n",
        "        st.session_state.voice_transcribed_text = \"\"\n",
        "        st.session_state.audio_processed = True\n",
        "\n",
        "        # Add user message\n",
        "        st.session_state.messages.append({\"role\": \"user\", \"content\": query})\n",
        "\n",
        "        with st.chat_message(\"user\"):\n",
        "            st.markdown(query)\n",
        "\n",
        "        st.session_state.pending_query = query\n",
        "        st.session_state.execute_pending = True\n",
        "        st.rerun()\n",
        "\n",
        "\n",
        "# Initialize Langfuse client\n",
        "if st.session_state.langfuse is None:\n",
        "    st.session_state.langfuse = initialize_langfuse()\n",
        "\n",
        "# Initialize RAG system automatically\n",
        "if st.session_state.rag_system is None:\n",
        "    load_rag_system()\n",
        "\n",
        "\n",
        "# Main App Layout\n",
        "st.title(\"🏥 TechCare Solutions Chatbot\")\n",
        "st.markdown(\"Your AI-powered medical assistant with advanced medical coding capabilities\")\n",
        "\n",
        "# # Display LangGraph status\n",
        "# if LANGGRAPH_ENABLED:\n",
        "#     st.success(\"✅ LangGraph Workflow Engine Active\")\n",
        "# else:\n",
        "#     st.warning(\"⚠️ LangGraph not available - using fallback mode\")\n",
        "\n",
        "# Main content container\n",
        "with st.container():\n",
        "    st.markdown('<div class=\"main-content\">', unsafe_allow_html=True)\n",
        "\n",
        "    # Sidebar Configuration\n",
        "    with st.sidebar:\n",
        "        # st.header(\"⚙️ Configuration\")\n",
        "\n",
        "        # # Display system status\n",
        "        # if st.session_state.rag_system:\n",
        "        #     if st.session_state.embeddings_loaded:\n",
        "        #         st.success(\"✅ RAG System Ready\")\n",
        "        #         st.info(f\"📊 Records loaded: {len(st.session_state.rag_system.medical_records)}\")\n",
        "\n",
        "        #         # LangGraph status in sidebar\n",
        "        #         if LANGGRAPH_ENABLED and st.session_state.rag_system.workflow_engine:\n",
        "        #             st.success(\"🔄 LangGraph Workflow Active\")\n",
        "        #         else:\n",
        "        #             st.warning(\"⚠️ Using Legacy Workflow\")\n",
        "        #     else:\n",
        "        #         st.warning(\"⚠️ RAG System initialized but embeddings not loaded\")\n",
        "        #         if st.button(\"🔥 Reload Embeddings\", type=\"primary\"):\n",
        "        #             load_rag_system()\n",
        "        # else:\n",
        "        #     st.error(\"❌ RAG System not initialized\")\n",
        "        #     if st.button(\"🚀 Initialize RAG System\", type=\"primary\"):\n",
        "        #         load_rag_system()\n",
        "\n",
        "        # # Langfuse status\n",
        "        # if st.session_state.langfuse:\n",
        "        #     st.success(\"✅ Langfuse Connected\")\n",
        "        # else:\n",
        "        #     st.error(\"❌ Langfuse Not Connected\")\n",
        "\n",
        "        # st.divider()\n",
        "\n",
        "        # Query settings\n",
        "        st.subheader(\"🎛️ Query Settings\")\n",
        "        confidence_threshold = st.slider(\n",
        "            \"Confidence Threshold\",\n",
        "            min_value=0.0,\n",
        "            max_value=1.0,\n",
        "            value=0.7,\n",
        "            step=0.01,\n",
        "            help=\"Minimum similarity score required to use RAG. Lower scores will fallback to Gemini.\"\n",
        "        )\n",
        "\n",
        "        top_k = st.slider(\n",
        "            \"Max Records to Retrieve\",\n",
        "            min_value=1,\n",
        "            max_value=10,\n",
        "            value=3,\n",
        "            step=5\n",
        "        )\n",
        "\n",
        "        st.divider()\n",
        "\n",
        "        # FAQ Section with Sample Queries\n",
        "        st.header(\"❓ FAQ - Sample Queries\")\n",
        "        st.markdown(\"Click on any query to execute it:\")\n",
        "\n",
        "        for query in SAMPLE_QUERIES:\n",
        "            if st.button(f\"💬 {query}\", key=f\"faq_{query}\", use_container_width=True):\n",
        "                handle_sample_query_selection(query)\n",
        "\n",
        "        st.divider()\n",
        "\n",
        "        # Document Upload Section\n",
        "        st.header(\"📄 Document Context\")\n",
        "        st.markdown(\"Upload medical documents for additional context\")\n",
        "\n",
        "        uploaded_files = st.file_uploader(\n",
        "            \"Choose files to add context\",\n",
        "            accept_multiple_files=True,\n",
        "            type=['pdf', 'docx', 'txt'],\n",
        "            key=\"sidebar_file_uploader\",\n",
        "            help=\"Upload PDF, Word documents, or text files\"\n",
        "        )\n",
        "\n",
        "        # Process and display uploaded files\n",
        "        if uploaded_files:\n",
        "            current_file_names = [f.name for f in uploaded_files]\n",
        "\n",
        "            # Check if files have changed\n",
        "            if set(current_file_names) != set(st.session_state.sidebar_files_processed.keys()):\n",
        "                with st.spinner(\"📋 Processing documents...\"):\n",
        "                    st.session_state.sidebar_files_processed = {}\n",
        "                    combined_text = \"\"\n",
        "\n",
        "                    for file in uploaded_files:\n",
        "                        file_info = process_single_file(file)\n",
        "                        st.session_state.sidebar_files_processed[file.name] = file_info\n",
        "\n",
        "                        if file_info[\"text\"] and not file_info[\"text\"].startswith(\"Unsupported\") and not file_info[\"text\"].startswith(\"Error\"):\n",
        "                            combined_text += f\"\\n\\n--- Content from {file.name} ---\\n{file_info['text']}\"\n",
        "\n",
        "                    st.session_state.uploaded_files_text = combined_text\n",
        "\n",
        "            # Display file information\n",
        "            st.subheader(\"📃 Uploaded Documents\")\n",
        "            for filename, file_info in st.session_state.sidebar_files_processed.items():\n",
        "                if file_info[\"status\"] == \"success\":\n",
        "                    with st.expander(f\"📄 {filename}\", expanded=False):\n",
        "                        st.markdown(f\"**Type:** {file_info['type']}\")\n",
        "                        st.markdown(f\"**Size:** {file_info['size']} bytes\")\n",
        "                        st.markdown(f\"**Text Length:** {file_info['text_length']} characters\")\n",
        "\n",
        "                        if file_info[\"text\"] and len(file_info[\"text\"]) > 100:\n",
        "                            st.text_area(\"Content Preview\", file_info[\"text\"][:500] + \"...\", height=100)\n",
        "                else:\n",
        "                    st.error(f\"❌ Failed to process: {filename}\")\n",
        "        else:\n",
        "            # Clear files if no files are uploaded\n",
        "            if st.session_state.sidebar_files_processed:\n",
        "                st.session_state.sidebar_files_processed = {}\n",
        "                st.session_state.uploaded_files_text = \"\"\n",
        "\n",
        "        # Langfuse section\n",
        "        st.divider()\n",
        "        st.header(\"📊 Langfuse Tracking\")\n",
        "        if st.session_state.langfuse:\n",
        "            st.markdown(\"**Status:** Connected ✅\")\n",
        "            st.markdown(\"**Tracking:** All responses tracked\")\n",
        "            langfuse_traces_url = \"https://cloud.langfuse.com/project/cmfiuk7380001ad074uio1la1/traces\"\n",
        "            st.markdown(f\"[View All Traces]({langfuse_traces_url})\")\n",
        "        else:\n",
        "            st.markdown(\"**Status:** Disconnected ❌\")\n",
        "\n",
        "    # Execute pending query if available\n",
        "    if st.session_state.execute_pending and st.session_state.pending_query:\n",
        "        user_query = st.session_state.pending_query\n",
        "\n",
        "        # Add user message to chat history\n",
        "        st.session_state.messages.append({\"role\": \"user\", \"content\": user_query})\n",
        "\n",
        "        with st.chat_message(\"user\"):\n",
        "            st.markdown(user_query)\n",
        "\n",
        "        # Execute RAG query (real response will replace processing bubble in next rerun)\n",
        "        execute_rag_query(user_query, confidence_threshold, top_k)\n",
        "\n",
        "        # Clear pending query\n",
        "        st.session_state.pending_query = \"\"\n",
        "        st.session_state.execute_pending = False\n",
        "        st.rerun()\n",
        "\n",
        "    # Display chat messages\n",
        "    for message in st.session_state.messages:\n",
        "        with st.chat_message(message[\"role\"]):\n",
        "            if message[\"role\"] == \"assistant\":\n",
        "                # Display the response\n",
        "                st.markdown(message[\"content\"])\n",
        "\n",
        "                # Display medical codes box if available\n",
        "                if message.get(\"medical_codes\"):\n",
        "                    display_medical_codes_box(message[\"medical_codes\"])\n",
        "\n",
        "                # Display workflow status if available\n",
        "                if message.get(\"workflow_status\"):\n",
        "                    display_workflow_status(message[\"workflow_status\"])\n",
        "\n",
        "                # Display query intent if available\n",
        "                if message.get(\"query_intent\") and message[\"query_intent\"] != 'general':\n",
        "                    st.info(f\"🎯 Query Intent: {message['query_intent']}\")\n",
        "\n",
        "                # Display Langfuse trace link if available\n",
        "                if message.get(\"trace_id\") and st.session_state.langfuse:\n",
        "                    trace_url = f\"https://cloud.langfuse.com/project/cmfiuk7380001ad074uio1la1/traces/{message['trace_id']}\"\n",
        "                    st.markdown(f\"[→ View Langfuse Trace]({trace_url})\")\n",
        "\n",
        "                # Always add audio playback for assistant responses (RAG + Gemini fallback)\n",
        "                if AUDIO_ENABLED:\n",
        "                    try:\n",
        "                        audio_base64 = text_to_speech_base64(message[\"content\"][:300])\n",
        "                        if audio_base64:\n",
        "                            audio_html = f\"\"\"\n",
        "                            <div class=\"audio-player\">\n",
        "                                <audio controls>\n",
        "                                    <source src=\"data:audio/mp3;base64,{audio_base64}\" type=\"audio/mp3\">\n",
        "                                    Your browser does not support the audio element.\n",
        "                                </audio>\n",
        "                            </div>\n",
        "                            \"\"\"\n",
        "                            st.markdown(audio_html, unsafe_allow_html=True)\n",
        "                    except Exception:\n",
        "                        pass\n",
        "\n",
        "            else:\n",
        "                st.markdown(message[\"content\"])\n",
        "\n",
        "    # Voice transcription feedback - improved to directly execute\n",
        "    if st.session_state.voice_transcribed_text and not st.session_state.audio_processed:\n",
        "        st.markdown(f\"\"\"\n",
        "        <div class=\"voice-input-feedback\">\n",
        "            <strong>🎤 Voice Input Transcribed:</strong><br>\n",
        "            \"{st.session_state.voice_transcribed_text}\"\n",
        "        </div>\n",
        "        \"\"\", unsafe_allow_html=True)\n",
        "\n",
        "        col1, col2, col3 = st.columns([2, 1, 1])\n",
        "        with col1:\n",
        "            if st.button(\"📤 Send Voice Message\", key=\"send_voice\", type=\"primary\"):\n",
        "                handle_voice_input()\n",
        "        with col2:\n",
        "            if st.button(\"🗑️ Clear\", key=\"clear_voice\"):\n",
        "                st.session_state.voice_transcribed_text = \"\"\n",
        "                st.session_state.audio_processed = True\n",
        "                st.rerun()\n",
        "        with col3:\n",
        "            if st.button(\"🎤 Record Again\", key=\"record_again\"):\n",
        "                st.session_state.voice_transcribed_text = \"\"\n",
        "                st.session_state.audio_processed = True\n",
        "                st.rerun()\n",
        "\n",
        "    st.markdown('</div>', unsafe_allow_html=True)\n",
        "\n",
        "# Fixed input interface at bottom\n",
        "with st.container():\n",
        "    # Context indicator - improved checking\n",
        "    docs_available, num_docs, combined_text = check_documents_available()\n",
        "    if docs_available:\n",
        "        st.info(f\"🔍 Context active: {num_docs} document(s) will be included in responses\")\n",
        "\n",
        "    # Input interface\n",
        "    col1, col2 = st.columns([9, 1])\n",
        "\n",
        "    with col1:\n",
        "        # Handle sample query selection\n",
        "        if st.session_state.selected_sample_query:\n",
        "            prompt = st.chat_input(\n",
        "                st.session_state.selected_sample_query if st.session_state.selected_sample_query else \"Ask a medical question...\",\n",
        "                key=f\"chat_input_{st.session_state.chat_input_key}\"\n",
        "            )\n",
        "            st.session_state.selected_sample_query = \"\"\n",
        "        else:\n",
        "            prompt = st.chat_input(\"Ask a medical question...\", key=f\"chat_input_{st.session_state.chat_input_key}\")\n",
        "\n",
        "    with col2:\n",
        "        if AUDIO_ENABLED and not st.session_state.voice_transcribed_text:\n",
        "            audio = audiorecorder(\n",
        "                start_prompt=\"🎤\",\n",
        "                stop_prompt=\"🔴\",\n",
        "                key=\"voice_recorder\",\n",
        "                show_visualizer=True\n",
        "            )\n",
        "\n",
        "            # Handle voice input\n",
        "            if len(audio) > 0 and not st.session_state.is_recording:\n",
        "                st.session_state.is_recording = True\n",
        "                with st.spinner(\"🎤 Transcribing audio...\"):\n",
        "                    transcribed_text = improved_voice_transcription(audio)\n",
        "                    if transcribed_text and not transcribed_text.startswith(\"Error\") and not transcribed_text.startswith(\"Audio features\") and not transcribed_text.startswith(\"Whisper model\"):\n",
        "                        st.session_state.voice_transcribed_text = transcribed_text\n",
        "                        st.session_state.audio_processed = False\n",
        "                        st.success(\"🎤 Voice transcribed successfully!\")\n",
        "                    else:\n",
        "                        st.error(f\"❌ {transcribed_text}\")\n",
        "\n",
        "                st.session_state.is_recording = False\n",
        "                st.rerun()\n",
        "        else:\n",
        "            st.markdown(\"🔇 Audio disabled\")\n",
        "\n",
        "# Process text input message\n",
        "if prompt:\n",
        "    # Reset states\n",
        "    st.session_state.audio_processed = True\n",
        "    st.session_state.voice_transcribed_text = \"\"\n",
        "    st.session_state.chat_input_key += 1\n",
        "\n",
        "    # Add user message to chat history\n",
        "    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "\n",
        "    with st.chat_message(\"user\"):\n",
        "        st.markdown(prompt)\n",
        "\n",
        "    # Generate assistant response\n",
        "    with st.chat_message(\"assistant\"):\n",
        "        execute_rag_query(prompt, confidence_threshold, top_k)\n",
        "\n",
        "    # Rerun to refresh the interface\n",
        "    st.rerun()\n",
        "\n",
        "# Footer\n",
        "st.markdown(\"---\")\n",
        "st.markdown(\"⚠️ **Disclaimer:** This is a demonstration system. Always consult healthcare professionals for medical advice.\")\n",
        "\n",
        "# Sidebar tracking info\n",
        "with st.sidebar:\n",
        "    if st.button(\"🗑️ Clear Chat\"):\n",
        "        st.session_state.messages = []\n",
        "        st.rerun()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BO-9PnE5QdXc",
        "outputId": "15519137-ad7f-411f-c17d-96988e0668ba"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ngrok_token = \"328GeQwD1llQcGkxP3EPmr1QS6t_2JS19i8o5SqhrMj8z7bLX\"  # Replace with your actual token\n",
        "\n",
        "# 4: Run Your App (With sharing - requires ngrok token)\n",
        "\n",
        "from pyngrok import ngrok\n",
        "\n",
        "import time\n",
        "\n",
        "import threading\n",
        "\n",
        "\n",
        "\n",
        "# Set your ngrok authentication token (replace ngrok_token with your actual token)\n",
        "\n",
        "ngrok.set_auth_token(ngrok_token)\n",
        "\n",
        "\n",
        "\n",
        "# Function to launch the Streamlit app using a system command\n",
        "\n",
        "def run_app():\n",
        "\n",
        "    !streamlit run app.py --server.headless true --server.port 8501\n",
        "\n",
        "\n",
        "\n",
        "# Terminate any active ngrok tunnels before starting a new one\n",
        "\n",
        "ngrok.kill()\n",
        "\n",
        "\n",
        "\n",
        "# Start the Streamlit app in a separate thread so the script can continue running\n",
        "\n",
        "app_thread = threading.Thread(target=run_app)\n",
        "\n",
        "app_thread.start()\n",
        "\n",
        "\n",
        "\n",
        "# Allow time for the Streamlit app to fully start before creating the tunnel\n",
        "\n",
        "time.sleep(10)\n",
        "\n",
        "\n",
        "\n",
        "# Create a public URL using ngrok and display it\n",
        "\n",
        "try:\n",
        "\n",
        "    public_url = ngrok.connect(8501)\n",
        "\n",
        "    print(\"🚀 Your app is live!\")\n",
        "\n",
        "    print(f\"🌐 Share this link: {public_url}\")\n",
        "\n",
        "    print(\"📱 Anyone can access your app with this link!\")\n",
        "\n",
        "except:\n",
        "\n",
        "    print(\"⚠️ Need ngrok token for sharing. App is running locally.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vYblIHs5QrQR",
        "outputId": "09b9337e-e6f6-4817-895b-18d6ae800fb0"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.55.248.60:8501\u001b[0m\n",
            "\u001b[0m\n",
            "🚀 Your app is live!\n",
            "🌐 Share this link: NgrokTunnel: \"https://2e9f6fde9867.ngrok-free.app\" -> \"http://localhost:8501\"\n",
            "📱 Anyone can access your app with this link!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Disable all the services"
      ],
      "metadata": {
        "id": "X4GTuw3JkFrg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# # --- FINAL CLEANUP SCRIPT FOR COLAB ---\n",
        "\n",
        "# PROJECT_ID = \"i-monolith-468706-i9\"\n",
        "# REGION = \"us-central1\"\n",
        "# BUCKET_NAME = \"bucket_jonah\"\n",
        "\n",
        "# # 1. Delete all objects in the bucket, then delete the bucket\n",
        "# !gcloud storage rm -r gs://$BUCKET_NAME --project=$PROJECT_ID || echo \"Bucket not found or already deleted\"\n",
        "# !gcloud storage buckets delete gs://$BUCKET_NAME --project=$PROJECT_ID --quiet || echo \"Bucket already deleted\"\n",
        "\n",
        "# # 2. Delete all Vertex AI endpoints\n",
        "# !for eid in $(gcloud ai endpoints list --region=$REGION --project=$PROJECT_ID --format=\"value(name)\"); do \\\n",
        "#     gcloud ai endpoints delete $eid --region=$REGION --project=$PROJECT_ID --quiet; \\\n",
        "# done\n",
        "\n",
        "# # 3. Delete all Vertex AI models\n",
        "# !for mid in $(gcloud ai models list --region=$REGION --project=$PROJECT_ID --format=\"value(name)\"); do \\\n",
        "#     gcloud ai models delete $mid --region=$REGION --project=$PROJECT_ID --quiet; \\\n",
        "# done\n",
        "\n",
        "# # 4. Stop all Vertex AI Workbench notebooks\n",
        "# !for nid in $(gcloud notebooks instances list --location=$REGION --project=$PROJECT_ID --format=\"value(name)\"); do \\\n",
        "#     gcloud notebooks instances stop $nid --location=$REGION --project=$PROJECT_ID --quiet; \\\n",
        "# done\n",
        "\n",
        "# # 5. Disable APIs (force disable dependent services)\n",
        "# !gcloud services disable aiplatform.googleapis.com --project=$PROJECT_ID --quiet || echo \"Vertex AI API already disabled\"\n",
        "# !gcloud services disable storage.googleapis.com --project=$PROJECT_ID --quiet --disable-dependent-services || echo \"Storage API already disabled\"\n",
        "\n",
        "# # 6. Disable the service account (ignore if not found)\n",
        "# !gcloud iam service-accounts disable vertex-sa@$PROJECT_ID.iam.gserviceaccount.com --project=$PROJECT_ID || echo \"Service account not found\"\n",
        "\n",
        "# print(\"✅ Final cleanup complete. All costing services stopped for project:\", PROJECT_ID)\n"
      ],
      "metadata": {
        "id": "pWBIvqfLkE0Z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}